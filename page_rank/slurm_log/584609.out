Running on host: tikgpu07
In directory: /home/janulm/janulm_agent_nodes/page_rank
Starting on: Thu Jan 5 23:03:04 CET 2023
SLURM_JOB_ID: 584609
alive1
alive2
alive3
alive2
running  0  :  8 random attention node_embedding 0.01 5 0.3 2
running  1  :  8 random attention node_embedding 0.01 5 0.3 2
running  2  :  8 random attention node_embedding 0.01 5 0.3 2
running  3  :  8 random attention node_embedding 0.01 5 0.3 2
running  4  :  8 random attention_reset node_embedding 0.01 5 0.3 2
running  5  :  8 random attention_reset node_embedding 0.01 5 0.3 2
running  6  :  8 random attention_reset node_embedding 0.01 5 0.3 2
running  7  :  8 random attention_reset node_embedding 0.01 5 0.3 2
running  8  :  8 random attention_reset node_embedding 0.01 5 0.3 3
running  9  :  8 random attention_reset node_embedding 0.01 5 0.3 3
running  10  :  8 random attention_reset node_embedding 0.01 5 0.3 3
running  11  :  8 random attention_reset node_embedding 0.01 5 0.3 3
running  12  :  8 random attention_reset node_embedding 0.01 5 0.3 4
running  13  :  8 random attention_reset node_embedding 0.01 5 0.3 4
running  14  :  8 random attention_reset node_embedding 0.01 5 0.3 4
running  15  :  8 random attention_reset node_embedding 0.01 5 0.3 4
running  16  :  8 random attention node_embedding 0.001 5 0.3 2
running  17  :  8 random attention node_embedding 0.001 5 0.3 2
running  18  :  8 random attention node_embedding 0.001 5 0.3 2
running  19  :  8 random attention node_embedding 0.001 5 0.3 2
running  20  :  8 random attention_reset node_embedding 0.001 5 0.3 2
running  21  :  8 random attention_reset node_embedding 0.001 5 0.3 2
running  22  :  8 random attention_reset node_embedding 0.001 5 0.3 2
running  23  :  8 random attention_reset node_embedding 0.001 5 0.3 2
running  24  :  8 random attention_reset node_embedding 0.001 5 0.3 3
running  25  :  8 random attention_reset node_embedding 0.001 5 0.3 3
running  26  :  8 random attention_reset node_embedding 0.001 5 0.3 3
running  27  :  8 random attention_reset node_embedding 0.001 5 0.3 3
running  28  :  8 random attention_reset node_embedding 0.001 5 0.3 4
running  29  :  8 random attention_reset node_embedding 0.001 5 0.3 4
running  30  :  8 random attention_reset node_embedding 0.001 5 0.3 4
running  31  :  8 random attention_reset node_embedding 0.001 5 0.3 4
running  32  :  8 random attention node_embedding 0.0001 5 0.3 2
running  33  :  8 random attention node_embedding 0.0001 5 0.3 2
running  34  :  8 random attention node_embedding 0.0001 5 0.3 2
running  35  :  8 random attention node_embedding 0.0001 5 0.3 2
running  36  :  8 random attention_reset node_embedding 0.0001 5 0.3 2
running  37  :  8 random attention_reset node_embedding 0.0001 5 0.3 2
running  38  :  8 random attention_reset node_embedding 0.0001 5 0.3 2
running  39  :  8 random attention_reset node_embedding 0.0001 5 0.3 2
running  40  :  8 random attention_reset node_embedding 0.0001 5 0.3 3
running  41  :  8 random attention_reset node_embedding 0.0001 5 0.3 3
running  42  :  8 random attention_reset node_embedding 0.0001 5 0.3 3
running  43  :  8 random attention_reset node_embedding 0.0001 5 0.3 3
running  44  :  8 random attention_reset node_embedding 0.0001 5 0.3 4
running  45  :  8 random attention_reset node_embedding 0.0001 5 0.3 4
running  46  :  8 random attention_reset node_embedding 0.0001 5 0.3 4
running  47  :  8 random attention_reset node_embedding 0.0001 5 0.3 4
running  48  :  8 random attention node_embedding 0.01 10 0.3 2
running  49  :  8 random attention node_embedding 0.01 10 0.3 2
running  50  :  8 random attention node_embedding 0.01 10 0.3 2
running  51  :  8 random attention node_embedding 0.01 10 0.3 2
running  52  :  8 random attention_reset node_embedding 0.01 10 0.3 2
running  53  :  8 random attention_reset node_embedding 0.01 10 0.3 2
running  54  :  8 random attention_reset node_embedding 0.01 10 0.3 2
running  55  :  8 random attention_reset node_embedding 0.01 10 0.3 2
running  56  :  8 random attention_reset node_embedding 0.01 10 0.3 3
running  57  :  8 random attention_reset node_embedding 0.01 10 0.3 3
running  58  :  8 random attention_reset node_embedding 0.01 10 0.3 3
running  59  :  8 random attention_reset node_embedding 0.01 10 0.3 3
running  60  :  8 random attention_reset node_embedding 0.01 10 0.3 4
running  61  :  8 random attention_reset node_embedding 0.01 10 0.3 4
running  62  :  8 random attention_reset node_embedding 0.01 10 0.3 4
running  63  :  8 random attention_reset node_embedding 0.01 10 0.3 4
running  64  :  8 random attention node_embedding 0.001 10 0.3 2
running  65  :  8 random attention node_embedding 0.001 10 0.3 2
running  66  :  8 random attention node_embedding 0.001 10 0.3 2
running  67  :  8 random attention node_embedding 0.001 10 0.3 2
running  68  :  8 random attention_reset node_embedding 0.001 10 0.3 2
running  69  :  8 random attention_reset node_embedding 0.001 10 0.3 2
running  70  :  8 random attention_reset node_embedding 0.001 10 0.3 2
running  71  :  8 random attention_reset node_embedding 0.001 10 0.3 2
running  72  :  8 random attention_reset node_embedding 0.001 10 0.3 3
running  73  :  8 random attention_reset node_embedding 0.001 10 0.3 3
running  74  :  8 random attention_reset node_embedding 0.001 10 0.3 3
running  75  :  8 random attention_reset node_embedding 0.001 10 0.3 3
running  76  :  8 random attention_reset node_embedding 0.001 10 0.3 4
running  77  :  8 random attention_reset node_embedding 0.001 10 0.3 4
running  78  :  8 random attention_reset node_embedding 0.001 10 0.3 4
running  79  :  8 random attention_reset node_embedding 0.001 10 0.3 4
running  80  :  8 random attention node_embedding 0.0001 10 0.3 2
running  81  :  8 random attention node_embedding 0.0001 10 0.3 2
running  82  :  8 random attention node_embedding 0.0001 10 0.3 2
running  83  :  8 random attention node_embedding 0.0001 10 0.3 2
running  84  :  8 random attention_reset node_embedding 0.0001 10 0.3 2
running  85  :  8 random attention_reset node_embedding 0.0001 10 0.3 2
running  86  :  8 random attention_reset node_embedding 0.0001 10 0.3 2
running  87  :  8 random attention_reset node_embedding 0.0001 10 0.3 2
running  88  :  8 random attention_reset node_embedding 0.0001 10 0.3 3
running  89  :  8 random attention_reset node_embedding 0.0001 10 0.3 3
running  90  :  8 random attention_reset node_embedding 0.0001 10 0.3 3
running  91  :  8 random attention_reset node_embedding 0.0001 10 0.3 3
running  92  :  8 random attention_reset node_embedding 0.0001 10 0.3 4
running  93  :  8 random attention_reset node_embedding 0.0001 10 0.3 4
running  94  :  8 random attention_reset node_embedding 0.0001 10 0.3 4
running  95  :  8 random attention_reset node_embedding 0.0001 10 0.3 4
running  96  :  8 random attention node_embedding 0.01 20 0.3 2
running  97  :  8 random attention node_embedding 0.01 20 0.3 2
running  98  :  8 random attention node_embedding 0.01 20 0.3 2
running  99  :  8 random attention node_embedding 0.01 20 0.3 2
running  100  :  8 random attention_reset node_embedding 0.01 20 0.3 2
running  101  :  8 random attention_reset node_embedding 0.01 20 0.3 2
running  102  :  8 random attention_reset node_embedding 0.01 20 0.3 2
running  103  :  8 random attention_reset node_embedding 0.01 20 0.3 2
running  104  :  8 random attention_reset node_embedding 0.01 20 0.3 3
running  105  :  8 random attention_reset node_embedding 0.01 20 0.3 3
running  106  :  8 random attention_reset node_embedding 0.01 20 0.3 3
running  107  :  8 random attention_reset node_embedding 0.01 20 0.3 3
running  108  :  8 random attention_reset node_embedding 0.01 20 0.3 4
running  109  :  8 random attention_reset node_embedding 0.01 20 0.3 4
running  110  :  8 random attention_reset node_embedding 0.01 20 0.3 4
running  111  :  8 random attention_reset node_embedding 0.01 20 0.3 4
running  112  :  8 random attention node_embedding 0.001 20 0.3 2
running  113  :  8 random attention node_embedding 0.001 20 0.3 2
running  114  :  8 random attention node_embedding 0.001 20 0.3 2
running  115  :  8 random attention node_embedding 0.001 20 0.3 2
running  116  :  8 random attention_reset node_embedding 0.001 20 0.3 2
running  117  :  8 random attention_reset node_embedding 0.001 20 0.3 2
running  118  :  8 random attention_reset node_embedding 0.001 20 0.3 2
running  119  :  8 random attention_reset node_embedding 0.001 20 0.3 2
running  120  :  8 random attention_reset node_embedding 0.001 20 0.3 3
running  121  :  8 random attention_reset node_embedding 0.001 20 0.3 3
running  122  :  8 random attention_reset node_embedding 0.001 20 0.3 3
running  123  :  8 random attention_reset node_embedding 0.001 20 0.3 3
running  124  :  8 random attention_reset node_embedding 0.001 20 0.3 4
running  125  :  8 random attention_reset node_embedding 0.001 20 0.3 4
running  126  :  8 random attention_reset node_embedding 0.001 20 0.3 4
running  127  :  8 random attention_reset node_embedding 0.001 20 0.3 4
running  128  :  8 random attention node_embedding 0.0001 20 0.3 2
running  129  :  8 random attention node_embedding 0.0001 20 0.3 2
running  130  :  8 random attention node_embedding 0.0001 20 0.3 2
running  131  :  8 random attention node_embedding 0.0001 20 0.3 2
running  132  :  8 random attention_reset node_embedding 0.0001 20 0.3 2
running  133  :  8 random attention_reset node_embedding 0.0001 20 0.3 2
running  134  :  8 random attention_reset node_embedding 0.0001 20 0.3 2
running  135  :  8 random attention_reset node_embedding 0.0001 20 0.3 2
running  136  :  8 random attention_reset node_embedding 0.0001 20 0.3 3
running  137  :  8 random attention_reset node_embedding 0.0001 20 0.3 3
running  138  :  8 random attention_reset node_embedding 0.0001 20 0.3 3
running  139  :  8 random attention_reset node_embedding 0.0001 20 0.3 3
running  140  :  8 random attention_reset node_embedding 0.0001 20 0.3 4
running  141  :  8 random attention_reset node_embedding 0.0001 20 0.3 4
running  142  :  8 random attention_reset node_embedding 0.0001 20 0.3 4
running  143  :  8 random attention_reset node_embedding 0.0001 20 0.3 4
running  144  :  16 random attention node_embedding 0.01 5 0.3 2
running  145  :  16 random attention node_embedding 0.01 5 0.3 2
running  146  :  16 random attention node_embedding 0.01 5 0.3 2
running  147  :  16 random attention node_embedding 0.01 5 0.3 2
running  148  :  16 random attention_reset node_embedding 0.01 5 0.3 2
running  149  :  16 random attention_reset node_embedding 0.01 5 0.3 2
running  150  :  16 random attention_reset node_embedding 0.01 5 0.3 2
running  151  :  16 random attention_reset node_embedding 0.01 5 0.3 2
running  152  :  16 random attention_reset node_embedding 0.01 5 0.3 3
running  153  :  16 random attention_reset node_embedding 0.01 5 0.3 3
running  154  :  16 random attention_reset node_embedding 0.01 5 0.3 3
running  155  :  16 random attention_reset node_embedding 0.01 5 0.3 3
running  156  :  16 random attention_reset node_embedding 0.01 5 0.3 4
running  157  :  16 random attention_reset node_embedding 0.01 5 0.3 4
running  158  :  16 random attention_reset node_embedding 0.01 5 0.3 4
running  159  :  16 random attention_reset node_embedding 0.01 5 0.3 4
running  160  :  16 random attention node_embedding 0.001 5 0.3 2
running  161  :  16 random attention node_embedding 0.001 5 0.3 2
running  162  :  16 random attention node_embedding 0.001 5 0.3 2
running  163  :  16 random attention node_embedding 0.001 5 0.3 2
running  164  :  16 random attention_reset node_embedding 0.001 5 0.3 2
running  165  :  16 random attention_reset node_embedding 0.001 5 0.3 2
running  166  :  16 random attention_reset node_embedding 0.001 5 0.3 2
running  167  :  16 random attention_reset node_embedding 0.001 5 0.3 2
running  168  :  16 random attention_reset node_embedding 0.001 5 0.3 3
running  169  :  16 random attention_reset node_embedding 0.001 5 0.3 3
running  170  :  16 random attention_reset node_embedding 0.001 5 0.3 3
running  171  :  16 random attention_reset node_embedding 0.001 5 0.3 3
running  172  :  16 random attention_reset node_embedding 0.001 5 0.3 4
running  173  :  16 random attention_reset node_embedding 0.001 5 0.3 4
running  174  :  16 random attention_reset node_embedding 0.001 5 0.3 4
running  175  :  16 random attention_reset node_embedding 0.001 5 0.3 4
running  176  :  16 random attention node_embedding 0.0001 5 0.3 2
running  177  :  16 random attention node_embedding 0.0001 5 0.3 2
running  178  :  16 random attention node_embedding 0.0001 5 0.3 2
running  179  :  16 random attention node_embedding 0.0001 5 0.3 2
running  180  :  16 random attention_reset node_embedding 0.0001 5 0.3 2
running  181  :  16 random attention_reset node_embedding 0.0001 5 0.3 2
running  182  :  16 random attention_reset node_embedding 0.0001 5 0.3 2
running  183  :  16 random attention_reset node_embedding 0.0001 5 0.3 2
running  184  :  16 random attention_reset node_embedding 0.0001 5 0.3 3
running  185  :  16 random attention_reset node_embedding 0.0001 5 0.3 3
running  186  :  16 random attention_reset node_embedding 0.0001 5 0.3 3
running  187  :  16 random attention_reset node_embedding 0.0001 5 0.3 3
running  188  :  16 random attention_reset node_embedding 0.0001 5 0.3 4
running  189  :  16 random attention_reset node_embedding 0.0001 5 0.3 4
running  190  :  16 random attention_reset node_embedding 0.0001 5 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=5, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0016, device='cuda:0') spear coeff:  tensor(0.0278, device='cuda:0')
running  191  :  16 random attention_reset node_embedding 0.0001 5 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=5, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(6.9211e-05, device='cuda:0') spear coeff:  tensor(-0.0365, device='cuda:0')
running  192  :  16 random attention node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.8456e-09, device='cuda:0') spear coeff:  tensor(0.0519, device='cuda:0')
running  193  :  16 random attention node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.4609e-08, device='cuda:0') spear coeff:  tensor(0.1342, device='cuda:0')
running  194  :  16 random attention node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.9532e-10, device='cuda:0') spear coeff:  tensor(0.2644, device='cuda:0')
running  195  :  16 random attention node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.2115e-07, device='cuda:0') spear coeff:  tensor(-0.0539, device='cuda:0')
running  196  :  16 random attention_reset node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.6572e-09, device='cuda:0') spear coeff:  tensor(0.2068, device='cuda:0')
running  197  :  16 random attention_reset node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.8602e-08, device='cuda:0') spear coeff:  tensor(0.2723, device='cuda:0')
running  198  :  16 random attention_reset node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.1628e-09, device='cuda:0') spear coeff:  tensor(0.2834, device='cuda:0')
running  199  :  16 random attention_reset node_embedding 0.01 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(9.5639e-06, device='cuda:0') spear coeff:  tensor(0.0803, device='cuda:0')
running  200  :  16 random attention_reset node_embedding 0.01 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(4.8443e-09, device='cuda:0') spear coeff:  tensor(0.0600, device='cuda:0')
running  201  :  16 random attention_reset node_embedding 0.01 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(6.4662e-08, device='cuda:0') spear coeff:  tensor(0.1529, device='cuda:0')
running  202  :  16 random attention_reset node_embedding 0.01 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(9.8095e-10, device='cuda:0') spear coeff:  tensor(0.2888, device='cuda:0')
running  203  :  16 random attention_reset node_embedding 0.01 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.3351e-06, device='cuda:0') spear coeff:  tensor(0.0148, device='cuda:0')
running  204  :  16 random attention_reset node_embedding 0.01 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.0573e-07, device='cuda:0') spear coeff:  tensor(0.0276, device='cuda:0')
running  205  :  16 random attention_reset node_embedding 0.01 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.6499e-08, device='cuda:0') spear coeff:  tensor(0.3318, device='cuda:0')
running  206  :  16 random attention_reset node_embedding 0.01 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.2301e-10, device='cuda:0') spear coeff:  tensor(0.3145, device='cuda:0')
running  207  :  16 random attention_reset node_embedding 0.01 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.0198e-06, device='cuda:0') spear coeff:  tensor(0.0169, device='cuda:0')
running  208  :  16 random attention node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.4233e-08, device='cuda:0') spear coeff:  tensor(0.0389, device='cuda:0')
running  209  :  16 random attention node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.4870e-05, device='cuda:0') spear coeff:  tensor(0.0064, device='cuda:0')
running  210  :  16 random attention node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.1883e-07, device='cuda:0') spear coeff:  tensor(-0.1156, device='cuda:0')
running  211  :  16 random attention node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.4013e-05, device='cuda:0') spear coeff:  tensor(-0.0592, device='cuda:0')
running  212  :  16 random attention_reset node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.2032e-06, device='cuda:0') spear coeff:  tensor(0.0495, device='cuda:0')
running  213  :  16 random attention_reset node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(4.4269e-05, device='cuda:0') spear coeff:  tensor(-0.0437, device='cuda:0')
running  214  :  16 random attention_reset node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.2338e-06, device='cuda:0') spear coeff:  tensor(-0.1188, device='cuda:0')
running  215  :  16 random attention_reset node_embedding 0.001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.7990e-05, device='cuda:0') spear coeff:  tensor(0.0681, device='cuda:0')
running  216  :  16 random attention_reset node_embedding 0.001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.2648e-06, device='cuda:0') spear coeff:  tensor(0.0455, device='cuda:0')
running  217  :  16 random attention_reset node_embedding 0.001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.6550e-05, device='cuda:0') spear coeff:  tensor(-0.0326, device='cuda:0')
running  218  :  16 random attention_reset node_embedding 0.001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.7435e-07, device='cuda:0') spear coeff:  tensor(-0.1293, device='cuda:0')
running  219  :  16 random attention_reset node_embedding 0.001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.4668e-05, device='cuda:0') spear coeff:  tensor(0.0190, device='cuda:0')
running  220  :  16 random attention_reset node_embedding 0.001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.8420e-06, device='cuda:0') spear coeff:  tensor(0.0399, device='cuda:0')
running  221  :  16 random attention_reset node_embedding 0.001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.4179e-05, device='cuda:0') spear coeff:  tensor(-0.0399, device='cuda:0')
running  222  :  16 random attention_reset node_embedding 0.001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(4.2297e-06, device='cuda:0') spear coeff:  tensor(0.2282, device='cuda:0')
running  223  :  16 random attention_reset node_embedding 0.001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.9860e-05, device='cuda:0') spear coeff:  tensor(0.0053, device='cuda:0')
running  224  :  16 random attention node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0007, device='cuda:0') spear coeff:  tensor(-0.0090, device='cuda:0')
running  225  :  16 random attention node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0254, device='cuda:0') spear coeff:  tensor(0.1639, device='cuda:0')
running  226  :  16 random attention node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0039, device='cuda:0') spear coeff:  tensor(0.0585, device='cuda:0')
running  227  :  16 random attention node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0029, device='cuda:0') spear coeff:  tensor(-0.0133, device='cuda:0')
running  228  :  16 random attention_reset node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0010, device='cuda:0') spear coeff:  tensor(0.0497, device='cuda:0')
running  229  :  16 random attention_reset node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0026, device='cuda:0') spear coeff:  tensor(-0.0413, device='cuda:0')
running  230  :  16 random attention_reset node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0030, device='cuda:0') spear coeff:  tensor(0.1034, device='cuda:0')
running  231  :  16 random attention_reset node_embedding 0.0001 10 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0060, device='cuda:0') spear coeff:  tensor(0.1346, device='cuda:0')
running  232  :  16 random attention_reset node_embedding 0.0001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0154, device='cuda:0') spear coeff:  tensor(0.1360, device='cuda:0')
running  233  :  16 random attention_reset node_embedding 0.0001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0024, device='cuda:0') spear coeff:  tensor(-0.0215, device='cuda:0')
running  234  :  16 random attention_reset node_embedding 0.0001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0039, device='cuda:0') spear coeff:  tensor(0.0903, device='cuda:0')
running  235  :  16 random attention_reset node_embedding 0.0001 10 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0059, device='cuda:0') spear coeff:  tensor(0.0687, device='cuda:0')
running  236  :  16 random attention_reset node_embedding 0.0001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0165, device='cuda:0') spear coeff:  tensor(0.1088, device='cuda:0')
running  237  :  16 random attention_reset node_embedding 0.0001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0535, device='cuda:0') spear coeff:  tensor(-0.0266, device='cuda:0')
running  238  :  16 random attention_reset node_embedding 0.0001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0029, device='cuda:0') spear coeff:  tensor(0.0209, device='cuda:0')
running  239  :  16 random attention_reset node_embedding 0.0001 10 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=10, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0079, device='cuda:0') spear coeff:  tensor(0.0532, device='cuda:0')
running  240  :  16 random attention node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.6512e-05, device='cuda:0') spear coeff:  tensor(0.0339, device='cuda:0')
running  241  :  16 random attention node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.2687e-08, device='cuda:0') spear coeff:  tensor(0.2837, device='cuda:0')
running  242  :  16 random attention node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.1760e-05, device='cuda:0') spear coeff:  tensor(0.3076, device='cuda:0')
running  243  :  16 random attention node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(5.0233e-06, device='cuda:0') spear coeff:  tensor(0.2596, device='cuda:0')
running  244  :  16 random attention_reset node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.1640e-05, device='cuda:0') spear coeff:  tensor(0.2746, device='cuda:0')
running  245  :  16 random attention_reset node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.8480e-06, device='cuda:0') spear coeff:  tensor(-0.0867, device='cuda:0')
running  246  :  16 random attention_reset node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 23.70 GiB total capacity; 21.55 GiB already allocated; 123.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  247  :  16 random attention_reset node_embedding 0.01 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 23.70 GiB total capacity; 21.20 GiB already allocated; 65.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  248  :  16 random attention_reset node_embedding 0.01 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(4.1688e-05, device='cuda:0') spear coeff:  tensor(0.3171, device='cuda:0')
running  249  :  16 random attention_reset node_embedding 0.01 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.0259e-05, device='cuda:0') spear coeff:  tensor(0.2562, device='cuda:0')
running  250  :  16 random attention_reset node_embedding 0.01 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 159.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  251  :  16 random attention_reset node_embedding 0.01 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.71 GiB already allocated; 75.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  252  :  16 random attention_reset node_embedding 0.01 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.6505e-05, device='cuda:0') spear coeff:  tensor(0.2135, device='cuda:0')
running  253  :  16 random attention_reset node_embedding 0.01 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.6980e-06, device='cuda:0') spear coeff:  tensor(-0.0685, device='cuda:0')
running  254  :  16 random attention_reset node_embedding 0.01 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 21.65 GiB already allocated; 3.69 MiB free; 21.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  255  :  16 random attention_reset node_embedding 0.01 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.59 GiB already allocated; 195.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  256  :  16 random attention node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(9.8315e-06, device='cuda:0') spear coeff:  tensor(0.1267, device='cuda:0')
running  257  :  16 random attention node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(7.0453e-05, device='cuda:0') spear coeff:  tensor(0.1865, device='cuda:0')
running  258  :  16 random attention node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0006, device='cuda:0') spear coeff:  tensor(-0.0938, device='cuda:0')
running  259  :  16 random attention node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0016, device='cuda:0') spear coeff:  tensor(0.0464, device='cuda:0')
running  260  :  16 random attention_reset node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(2.0854e-05, device='cuda:0') spear coeff:  tensor(0.0916, device='cuda:0')
running  261  :  16 random attention_reset node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.3208e-06, device='cuda:0') spear coeff:  tensor(-0.0122, device='cuda:0')
running  262  :  16 random attention_reset node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 23.70 GiB total capacity; 21.57 GiB already allocated; 87.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  263  :  16 random attention_reset node_embedding 0.001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.70 GiB total capacity; 21.69 GiB already allocated; 19.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  264  :  16 random attention_reset node_embedding 0.001 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(1.2314e-05, device='cuda:0') spear coeff:  tensor(0.1105, device='cuda:0')
running  265  :  16 random attention_reset node_embedding 0.001 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(3.5303e-07, device='cuda:0') spear coeff:  tensor(-0.0189, device='cuda:0')
running  266  :  16 random attention_reset node_embedding 0.001 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.36 GiB already allocated; 147.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  267  :  16 random attention_reset node_embedding 0.001 20 0.3 3
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.76 GiB already allocated; 1.69 MiB free; 21.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  268  :  16 random attention_reset node_embedding 0.001 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0001, device='cuda:0') spear coeff:  tensor(0.1074, device='cuda:0')
running  269  :  16 random attention_reset node_embedding 0.001 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(7.0570e-07, device='cuda:0') spear coeff:  tensor(0.0220, device='cuda:0')
running  270  :  16 random attention_reset node_embedding 0.001 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 23.70 GiB total capacity; 21.63 GiB already allocated; 7.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  271  :  16 random attention_reset node_embedding 0.001 20 0.3 4
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=4, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.48 GiB already allocated; 307.69 MiB free; 21.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  272  :  16 random attention node_embedding 0.0001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0010, device='cuda:0') spear coeff:  tensor(0.1118, device='cuda:0')
running  273  :  16 random attention node_embedding 0.0001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0029, device='cuda:0') spear coeff:  tensor(-0.1307, device='cuda:0')
running  274  :  16 random attention node_embedding 0.0001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=254014, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0602, device='cuda:0') spear coeff:  tensor(-0.0664, device='cuda:0')
running  275  :  16 random attention node_embedding 0.0001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=338686, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0342, device='cuda:0') spear coeff:  tensor(0.0448, device='cuda:0')
running  276  :  16 random attention_reset node_embedding 0.0001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=84671, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
test_loss:  tensor(0.0007, device='cuda:0') spear coeff:  tensor(0.0917, device='cuda:0')
running  277  :  16 random attention_reset node_embedding 0.0001 20 0.3 2
Namespace(init_strat=<InitStrategy.random: 'random'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, classification=False, device=0, seed=0, hidden_units=16, num_agents=169343, num_steps=20, self_loops=True, epochs=4000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogb_arxiv')
