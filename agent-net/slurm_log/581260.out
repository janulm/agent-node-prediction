Running on host: tikgpu09
In directory: /home/janulm/janulm_agent_nodes/agent-net
Starting on: Wed Dec 21 18:56:23 CET 2022
SLURM_JOB_ID: 581260
running  0  :  32 one_to_one attention agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6284591486122256
running  1  :  32 one_to_one attention agent_start 0.01 3 0.0 2
skipped because reset not used
running  2  :  32 one_to_one attention agent_start 0.01 3 0.0 3
skipped because reset not used
running  3  :  32 one_to_one attention_reset agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6242824517005123
running  4  :  32 one_to_one attention_reset agent_start 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6619138736291998
running  5  :  32 one_to_one attention_reset agent_start 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6538691027302841
running  6  :  32 one_to_one bias_attention agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.10 MiB already allocated; 21.43 GiB free; 460.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  7  :  32 one_to_one bias_attention agent_start 0.01 3 0.0 2
skipped because reset not used
running  8  :  32 one_to_one bias_attention agent_start 0.01 3 0.0 3
skipped because reset not used
running  9  :  32 one_to_one bias_attention_reset agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  10  :  32 one_to_one bias_attention_reset agent_start 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  11  :  32 one_to_one bias_attention_reset agent_start 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  12  :  32 one_to_one attention last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.563751208773121
running  13  :  32 one_to_one attention last_agent_visited 0.01 3 0.0 2
skipped because reset not used
running  14  :  32 one_to_one attention last_agent_visited 0.01 3 0.0 3
skipped because reset not used
running  15  :  32 one_to_one attention_reset last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6030903442174351
running  16  :  32 one_to_one attention_reset last_agent_visited 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6600621360821348
running  17  :  32 one_to_one attention_reset last_agent_visited 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5669814620496677
running  18  :  32 one_to_one bias_attention last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  19  :  32 one_to_one bias_attention last_agent_visited 0.01 3 0.0 2
skipped because reset not used
running  20  :  32 one_to_one bias_attention last_agent_visited 0.01 3 0.0 3
skipped because reset not used
running  21  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  22  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  23  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  24  :  32 one_to_one attention node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6431907495422093
running  25  :  32 one_to_one attention node_embedding 0.01 3 0.0 2
skipped because reset not used
running  26  :  32 one_to_one attention node_embedding 0.01 3 0.0 3
skipped because reset not used
running  27  :  32 one_to_one attention_reset node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6421620064605066
running  28  :  32 one_to_one attention_reset node_embedding 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6573462543464396
running  29  :  32 one_to_one attention_reset node_embedding 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6406394666995865
running  30  :  32 one_to_one bias_attention node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  31  :  32 one_to_one bias_attention node_embedding 0.01 3 0.0 2
skipped because reset not used
running  32  :  32 one_to_one bias_attention node_embedding 0.01 3 0.0 3
skipped because reset not used
running  33  :  32 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  34  :  32 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  35  :  32 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  36  :  32 one_to_one attention agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6364833446495072
running  37  :  32 one_to_one attention agent_start 0.001 3 0.0 2
skipped because reset not used
running  38  :  32 one_to_one attention agent_start 0.001 3 0.0 3
skipped because reset not used
running  39  :  32 one_to_one attention_reset agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.65232598810773
running  40  :  32 one_to_one attention_reset agent_start 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6451659362590787
running  41  :  32 one_to_one attention_reset agent_start 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6388288788757895
running  42  :  32 one_to_one bias_attention agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  43  :  32 one_to_one bias_attention agent_start 0.001 3 0.0 2
skipped because reset not used
running  44  :  32 one_to_one bias_attention agent_start 0.001 3 0.0 3
skipped because reset not used
running  45  :  32 one_to_one bias_attention_reset agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  46  :  32 one_to_one bias_attention_reset agent_start 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  47  :  32 one_to_one bias_attention_reset agent_start 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  48  :  32 one_to_one attention last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5359339958438779
running  49  :  32 one_to_one attention last_agent_visited 0.001 3 0.0 2
skipped because reset not used
running  50  :  32 one_to_one attention last_agent_visited 0.001 3 0.0 3
skipped because reset not used
running  51  :  32 one_to_one attention_reset last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6160525070468901
running  52  :  32 one_to_one attention_reset last_agent_visited 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6504331008373969
running  53  :  32 one_to_one attention_reset last_agent_visited 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5135691212476596
running  54  :  32 one_to_one bias_attention last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  55  :  32 one_to_one bias_attention last_agent_visited 0.001 3 0.0 2
skipped because reset not used
running  56  :  32 one_to_one bias_attention last_agent_visited 0.001 3 0.0 3
skipped because reset not used
running  57  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  58  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  59  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  60  :  32 one_to_one attention node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6448778881962018
running  61  :  32 one_to_one attention node_embedding 0.001 3 0.0 2
skipped because reset not used
running  62  :  32 one_to_one attention node_embedding 0.001 3 0.0 3
skipped because reset not used
running  63  :  32 one_to_one attention_reset node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6633129642203156
running  64  :  32 one_to_one attention_reset node_embedding 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6559265888936897
running  65  :  32 one_to_one attention_reset node_embedding 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6443635166553505
running  66  :  32 one_to_one bias_attention node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  67  :  32 one_to_one bias_attention node_embedding 0.001 3 0.0 2
skipped because reset not used
running  68  :  32 one_to_one bias_attention node_embedding 0.001 3 0.0 3
skipped because reset not used
running  69  :  32 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  70  :  32 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  71  :  32 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  72  :  32 one_to_one attention agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4838384461864494
running  73  :  32 one_to_one attention agent_start 0.0001 3 0.0 2
skipped because reset not used
running  74  :  32 one_to_one attention agent_start 0.0001 3 0.0 3
skipped because reset not used
running  75  :  32 one_to_one attention_reset agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5384235541015987
running  76  :  32 one_to_one attention_reset agent_start 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5157089068576014
running  77  :  32 one_to_one attention_reset agent_start 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4878299693434562
running  78  :  32 one_to_one bias_attention agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  79  :  32 one_to_one bias_attention agent_start 0.0001 3 0.0 2
skipped because reset not used
running  80  :  32 one_to_one bias_attention agent_start 0.0001 3 0.0 3
skipped because reset not used
running  81  :  32 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  82  :  32 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  83  :  32 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  84  :  32 one_to_one attention last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.42670205542867723
running  85  :  32 one_to_one attention last_agent_visited 0.0001 3 0.0 2
skipped because reset not used
running  86  :  32 one_to_one attention last_agent_visited 0.0001 3 0.0 3
skipped because reset not used
running  87  :  32 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5444931382836451
running  88  :  32 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5141452173734131
running  89  :  32 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4282451700512314
running  90  :  32 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  91  :  32 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 2
skipped because reset not used
running  92  :  32 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 3
skipped because reset not used
running  93  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  94  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  95  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  96  :  32 one_to_one attention node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5568174804024443
running  97  :  32 one_to_one attention node_embedding 0.0001 3 0.0 2
skipped because reset not used
running  98  :  32 one_to_one attention node_embedding 0.0001 3 0.0 3
skipped because reset not used
running  99  :  32 one_to_one attention_reset node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5733390942945908
running  100  :  32 one_to_one attention_reset node_embedding 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5801287986338292
running  101  :  32 one_to_one attention_reset node_embedding 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5558710367672778
running  102  :  32 one_to_one bias_attention node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  103  :  32 one_to_one bias_attention node_embedding 0.0001 3 0.0 2
skipped because reset not used
running  104  :  32 one_to_one bias_attention node_embedding 0.0001 3 0.0 3
skipped because reset not used
running  105  :  32 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  106  :  32 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  107  :  32 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  108  :  32 one_to_one attention agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6153323868896982
running  109  :  32 one_to_one attention agent_start 0.01 6 0.0 2
skipped because reset not used
running  110  :  32 one_to_one attention agent_start 0.01 6 0.0 3
skipped because reset not used
running  111  :  32 one_to_one attention_reset agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6155998600909409
running  112  :  32 one_to_one attention_reset agent_start 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5874328745139189
running  113  :  32 one_to_one attention_reset agent_start 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5270045058946978
running  114  :  32 one_to_one bias_attention agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  115  :  32 one_to_one bias_attention agent_start 0.01 6 0.0 2
skipped because reset not used
running  116  :  32 one_to_one bias_attention agent_start 0.01 6 0.0 3
skipped because reset not used
running  117  :  32 one_to_one bias_attention_reset agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  118  :  32 one_to_one bias_attention_reset agent_start 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  119  :  32 one_to_one bias_attention_reset agent_start 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  120  :  32 one_to_one attention last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3892146575314281
running  121  :  32 one_to_one attention last_agent_visited 0.01 6 0.0 2
skipped because reset not used
running  122  :  32 one_to_one attention last_agent_visited 0.01 6 0.0 3
skipped because reset not used
running  123  :  32 one_to_one attention_reset last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6035224163117503
running  124  :  32 one_to_one attention_reset last_agent_visited 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6135012242042672
running  125  :  32 one_to_one attention_reset last_agent_visited 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5157294817192354
running  126  :  32 one_to_one bias_attention last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  127  :  32 one_to_one bias_attention last_agent_visited 0.01 6 0.0 2
skipped because reset not used
running  128  :  32 one_to_one bias_attention last_agent_visited 0.01 6 0.0 3
skipped because reset not used
running  129  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  130  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  131  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  132  :  32 one_to_one attention node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6402485443285394
running  133  :  32 one_to_one attention node_embedding 0.01 6 0.0 2
skipped because reset not used
running  134  :  32 one_to_one attention node_embedding 0.01 6 0.0 3
skipped because reset not used
running  135  :  32 one_to_one attention_reset node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6335205645742032
running  136  :  32 one_to_one attention_reset node_embedding 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6429850009258687
running  137  :  32 one_to_one attention_reset node_embedding 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6349608048885871
running  138  :  32 one_to_one bias_attention node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  139  :  32 one_to_one bias_attention node_embedding 0.01 6 0.0 2
skipped because reset not used
running  140  :  32 one_to_one bias_attention node_embedding 0.01 6 0.0 3
skipped because reset not used
running  141  :  32 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  142  :  32 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  143  :  32 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  144  :  32 one_to_one attention agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6150649136884554
running  145  :  32 one_to_one attention agent_start 0.001 6 0.0 2
skipped because reset not used
running  146  :  32 one_to_one attention agent_start 0.001 6 0.0 3
skipped because reset not used
running  147  :  32 one_to_one attention_reset agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.630496059913997
running  148  :  32 one_to_one attention_reset agent_start 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6331913667880583
running  149  :  32 one_to_one attention_reset agent_start 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6254757936752875
running  150  :  32 one_to_one bias_attention agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  151  :  32 one_to_one bias_attention agent_start 0.001 6 0.0 2
skipped because reset not used
running  152  :  32 one_to_one bias_attention agent_start 0.001 6 0.0 3
skipped because reset not used
running  153  :  32 one_to_one bias_attention_reset agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  154  :  32 one_to_one bias_attention_reset agent_start 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  155  :  32 one_to_one bias_attention_reset agent_start 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  156  :  32 one_to_one attention last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5310989033598749
running  157  :  32 one_to_one attention last_agent_visited 0.001 6 0.0 2
skipped because reset not used
running  158  :  32 one_to_one attention last_agent_visited 0.001 6 0.0 3
skipped because reset not used
running  159  :  32 one_to_one attention_reset last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6309898565932144
running  160  :  32 one_to_one attention_reset last_agent_visited 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.63380861263708
running  161  :  32 one_to_one attention_reset last_agent_visited 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5705203382507252
running  162  :  32 one_to_one bias_attention last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  163  :  32 one_to_one bias_attention last_agent_visited 0.001 6 0.0 2
skipped because reset not used
running  164  :  32 one_to_one bias_attention last_agent_visited 0.001 6 0.0 3
skipped because reset not used
running  165  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  166  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  167  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  168  :  32 one_to_one attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6313396292409934
running  169  :  32 one_to_one attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  170  :  32 one_to_one attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  171  :  32 one_to_one attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6601855852519392
running  172  :  32 one_to_one attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6452482357056148
running  173  :  32 one_to_one attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6446104149949592
running  174  :  32 one_to_one bias_attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  175  :  32 one_to_one bias_attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  176  :  32 one_to_one bias_attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  177  :  32 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  178  :  32 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  179  :  32 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  180  :  32 one_to_one attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.39057259839927577
running  181  :  32 one_to_one attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  182  :  32 one_to_one attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  183  :  32 one_to_one attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.43919099644054893
running  184  :  32 one_to_one attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.43015863218319855
running  185  :  32 one_to_one attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4274633253091373
running  186  :  32 one_to_one bias_attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  187  :  32 one_to_one bias_attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  188  :  32 one_to_one bias_attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  189  :  32 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  190  :  32 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  191  :  32 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  192  :  32 one_to_one attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3420364998045388
running  193  :  32 one_to_one attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  194  :  32 one_to_one attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  195  :  32 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.43923214616381706
running  196  :  32 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4279365471267206
running  197  :  32 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.41633232516511326
running  198  :  32 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  199  :  32 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  200  :  32 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  201  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  202  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  203  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  204  :  32 one_to_one attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5034874390469725
running  205  :  32 one_to_one attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  206  :  32 one_to_one attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  207  :  32 one_to_one attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5508713453902022
running  208  :  32 one_to_one attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5355019237495627
running  209  :  32 one_to_one attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5332181141081827
running  210  :  32 one_to_one bias_attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  211  :  32 one_to_one bias_attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  212  :  32 one_to_one bias_attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  213  :  32 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  214  :  32 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  215  :  32 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  216  :  32 one_to_one attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5741620887599531
running  217  :  32 one_to_one attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  218  :  32 one_to_one attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  219  :  32 one_to_one attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5665905396786206
running  220  :  32 one_to_one attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5539781494969447
running  221  :  32 one_to_one attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5563236837232269
running  222  :  32 one_to_one bias_attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  223  :  32 one_to_one bias_attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  224  :  32 one_to_one bias_attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  225  :  32 one_to_one bias_attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  226  :  32 one_to_one bias_attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  227  :  32 one_to_one bias_attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  228  :  32 one_to_one attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.42738102586260107
running  229  :  32 one_to_one attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  230  :  32 one_to_one attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  231  :  32 one_to_one attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5705820628356274
running  232  :  32 one_to_one attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5071291895562002
running  233  :  32 one_to_one attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4632430096907598
running  234  :  32 one_to_one bias_attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  235  :  32 one_to_one bias_attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  236  :  32 one_to_one bias_attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  237  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  238  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  239  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  240  :  32 one_to_one attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6304137604674609
running  241  :  32 one_to_one attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  242  :  32 one_to_one attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  243  :  32 one_to_one attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.623047960002469
running  244  :  32 one_to_one attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6325123963541345
running  245  :  32 one_to_one attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6051066806575726
running  246  :  32 one_to_one bias_attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  247  :  32 one_to_one bias_attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  248  :  32 one_to_one bias_attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  249  :  32 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  250  :  32 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  251  :  32 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  252  :  32 one_to_one attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6251054461658745
running  253  :  32 one_to_one attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  254  :  32 one_to_one attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  255  :  32 one_to_one attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6044688599469169
running  256  :  32 one_to_one attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6182128675184659
running  257  :  32 one_to_one attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6085221076888258
running  258  :  32 one_to_one bias_attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  259  :  32 one_to_one bias_attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  260  :  32 one_to_one bias_attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  261  :  32 one_to_one bias_attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  262  :  32 one_to_one bias_attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  263  :  32 one_to_one bias_attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  264  :  32 one_to_one attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.48571075859514845
running  265  :  32 one_to_one attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  266  :  32 one_to_one attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  267  :  32 one_to_one attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5660144435528671
running  268  :  32 one_to_one attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6146945661790425
running  269  :  32 one_to_one attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5784828097031047
running  270  :  32 one_to_one bias_attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  271  :  32 one_to_one bias_attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  272  :  32 one_to_one bias_attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  273  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  274  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  275  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  276  :  32 one_to_one attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6312779046560912
running  277  :  32 one_to_one attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  278  :  32 one_to_one attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  279  :  32 one_to_one attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.627883052486472
running  280  :  32 one_to_one attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6294673168322943
running  281  :  32 one_to_one attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6328415941402794
running  282  :  32 one_to_one bias_attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  283  :  32 one_to_one bias_attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  284  :  32 one_to_one bias_attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  285  :  32 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  286  :  32 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  287  :  32 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  288  :  32 one_to_one attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.40158014937349545
running  289  :  32 one_to_one attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  290  :  32 one_to_one attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  291  :  32 one_to_one attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.457749521634467
running  292  :  32 one_to_one attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4588811390243401
running  293  :  32 one_to_one attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4521120095467358
running  294  :  32 one_to_one bias_attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  295  :  32 one_to_one bias_attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  296  :  32 one_to_one bias_attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  297  :  32 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  298  :  32 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  299  :  32 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  300  :  32 one_to_one attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.2808468613048577
running  301  :  32 one_to_one attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  302  :  32 one_to_one attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  303  :  32 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.48824146657613726
running  304  :  32 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4571734255087134
running  305  :  32 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.44931382836450423
running  306  :  32 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  307  :  32 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  308  :  32 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  309  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  310  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  311  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  312  :  32 one_to_one attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.45729687467851776
running  313  :  32 one_to_one attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  314  :  32 one_to_one attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  315  :  32 one_to_one attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4576260724646627
running  316  :  32 one_to_one attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.44334711849062813
running  317  :  32 one_to_one attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.46373680636997716
running  318  :  32 one_to_one bias_attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  319  :  32 one_to_one bias_attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  320  :  32 one_to_one bias_attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  321  :  32 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  322  :  32 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  323  :  32 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  324  :  32 one_to_one attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5706437874205296
running  325  :  32 one_to_one attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  326  :  32 one_to_one attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  327  :  32 one_to_one attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 330.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 283.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  328  :  32 one_to_one attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 266.00 MiB (GPU 0; 23.70 GiB total capacity; 19.51 GiB already allocated; 29.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  329  :  32 one_to_one attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.73 GiB already allocated; 299.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  330  :  32 one_to_one bias_attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  331  :  32 one_to_one bias_attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  332  :  32 one_to_one bias_attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  333  :  32 one_to_one bias_attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  334  :  32 one_to_one bias_attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  335  :  32 one_to_one bias_attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  336  :  32 one_to_one attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.73 GiB already allocated; 9.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  337  :  32 one_to_one attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  338  :  32 one_to_one attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  339  :  32 one_to_one attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.39 GiB already allocated; 229.69 MiB free; 21.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  340  :  32 one_to_one attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 268.00 MiB (GPU 0; 23.70 GiB total capacity; 20.81 GiB already allocated; 237.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  341  :  32 one_to_one attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.53 GiB already allocated; 119.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  342  :  32 one_to_one bias_attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  343  :  32 one_to_one bias_attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  344  :  32 one_to_one bias_attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  345  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  346  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  347  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  348  :  32 one_to_one attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.36 GiB already allocated; 29.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  349  :  32 one_to_one attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  350  :  32 one_to_one attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  351  :  32 one_to_one attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.70 GiB total capacity; 21.41 GiB already allocated; 13.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  352  :  32 one_to_one attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.25 GiB already allocated; 21.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  353  :  32 one_to_one attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  354  :  32 one_to_one bias_attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  355  :  32 one_to_one bias_attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  356  :  32 one_to_one bias_attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  357  :  32 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  358  :  32 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  359  :  32 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  360  :  32 one_to_one attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.64 GiB already allocated; 101.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  361  :  32 one_to_one attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  362  :  32 one_to_one attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  363  :  32 one_to_one attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.27 GiB already allocated; 3.69 MiB free; 21.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  364  :  32 one_to_one attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.27 GiB already allocated; 11.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  365  :  32 one_to_one attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 13.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  366  :  32 one_to_one bias_attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.42 GiB free; 472.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  367  :  32 one_to_one bias_attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  368  :  32 one_to_one bias_attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  369  :  32 one_to_one bias_attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.42 GiB free; 472.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  370  :  32 one_to_one bias_attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.42 GiB free; 472.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  371  :  32 one_to_one bias_attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.42 GiB free; 472.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  372  :  32 one_to_one attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.70 GiB total capacity; 21.72 GiB already allocated; 35.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  373  :  32 one_to_one attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  374  :  32 one_to_one attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  375  :  32 one_to_one attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 207.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  376  :  32 one_to_one attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.70 GiB total capacity; 21.39 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  377  :  32 one_to_one attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 21.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  378  :  32 one_to_one bias_attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  379  :  32 one_to_one bias_attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  380  :  32 one_to_one bias_attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  381  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  382  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  383  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  384  :  32 one_to_one attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6209698989774294
running  385  :  32 one_to_one attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  386  :  32 one_to_one attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  387  :  32 one_to_one attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.08 GiB already allocated; 127.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  388  :  32 one_to_one attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.95 GiB already allocated; 133.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  389  :  32 one_to_one attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.05 GiB already allocated; 135.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  390  :  32 one_to_one bias_attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  391  :  32 one_to_one bias_attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  392  :  32 one_to_one bias_attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  393  :  32 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  394  :  32 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  395  :  32 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  396  :  32 one_to_one attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 10, in <backward op>

            def backward(grad_output):
                grad_self = (grad_output * other)._grad_sum_to_size(self_size)
                             ~~~~~~~~~~~~~~~~~~~ <--- HERE
                grad_other = (grad_output * self)._grad_sum_to_size(other_size)
                return grad_self, grad_other
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.39 GiB already allocated; 129.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  397  :  32 one_to_one attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  398  :  32 one_to_one attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  399  :  32 one_to_one attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 23.70 GiB total capacity; 21.42 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  400  :  32 one_to_one attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.27 GiB already allocated; 31.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  401  :  32 one_to_one attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 33.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  402  :  32 one_to_one bias_attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.64 MiB already allocated; 21.43 GiB free; 466.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  403  :  32 one_to_one bias_attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  404  :  32 one_to_one bias_attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  405  :  32 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.64 MiB already allocated; 21.43 GiB free; 466.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  406  :  32 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.64 MiB already allocated; 21.43 GiB free; 466.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  407  :  32 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.64 MiB already allocated; 21.43 GiB free; 466.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  408  :  32 one_to_one attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.037384523589078865
running  409  :  32 one_to_one attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  410  :  32 one_to_one attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  411  :  32 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.39 GiB already allocated; 213.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  412  :  32 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.42 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  413  :  32 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  414  :  32 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  415  :  32 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  416  :  32 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  417  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  418  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  419  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  420  :  32 one_to_one attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4072588111844948
running  421  :  32 one_to_one attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  422  :  32 one_to_one attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  423  :  32 one_to_one attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 330.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 295.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  424  :  32 one_to_one attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.93 GiB already allocated; 297.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  425  :  32 one_to_one attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.07 GiB already allocated; 299.69 MiB free; 21.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  426  :  32 one_to_one bias_attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  427  :  32 one_to_one bias_attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  428  :  32 one_to_one bias_attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  429  :  32 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  430  :  32 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  431  :  32 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  432  :  32 one_to_one attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6573668292080735
running  433  :  32 one_to_one attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  434  :  32 one_to_one attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  435  :  32 one_to_one attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6741147665781948
running  436  :  32 one_to_one attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6755344320309445
running  437  :  32 one_to_one attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6573874040697076
running  438  :  32 one_to_one bias_attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  439  :  32 one_to_one bias_attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  440  :  32 one_to_one bias_attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  441  :  32 one_to_one bias_attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  442  :  32 one_to_one bias_attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  443  :  32 one_to_one bias_attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  444  :  32 one_to_one attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5370038886488488
running  445  :  32 one_to_one attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  446  :  32 one_to_one attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  447  :  32 one_to_one attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5795732773697096
running  448  :  32 one_to_one attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.673826718515318
running  449  :  32 one_to_one attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.48066991749480487
running  450  :  32 one_to_one bias_attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  451  :  32 one_to_one bias_attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  452  :  32 one_to_one bias_attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  453  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  454  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  455  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  456  :  32 one_to_one attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6387260045676193
running  457  :  32 one_to_one attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  458  :  32 one_to_one attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  459  :  32 one_to_one attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6519350657366829
running  460  :  32 one_to_one attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6558031397238854
running  461  :  32 one_to_one attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.640083945435467
running  462  :  32 one_to_one bias_attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  463  :  32 one_to_one bias_attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  464  :  32 one_to_one bias_attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  465  :  32 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  466  :  32 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  467  :  32 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  468  :  32 one_to_one attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6168960763738864
running  469  :  32 one_to_one attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  470  :  32 one_to_one attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  471  :  32 one_to_one attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6455980083533939
running  472  :  32 one_to_one attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6441371931773758
running  473  :  32 one_to_one attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6163405551097669
running  474  :  32 one_to_one bias_attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  475  :  32 one_to_one bias_attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  476  :  32 one_to_one bias_attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  477  :  32 one_to_one bias_attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  478  :  32 one_to_one bias_attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  479  :  32 one_to_one bias_attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  480  :  32 one_to_one attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4953192189782524
running  481  :  32 one_to_one attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  482  :  32 one_to_one attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  483  :  32 one_to_one attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5852519391807091
running  484  :  32 one_to_one attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6476349196551653
running  485  :  32 one_to_one attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.49050470135588337
running  486  :  32 one_to_one bias_attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  487  :  32 one_to_one bias_attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  488  :  32 one_to_one bias_attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  489  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  490  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  491  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  492  :  32 one_to_one attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6224512890150814
running  493  :  32 one_to_one attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  494  :  32 one_to_one attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  495  :  32 one_to_one attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.64491903791947
running  496  :  32 one_to_one attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6462152542024154
running  497  :  32 one_to_one attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6240767030841717
running  498  :  32 one_to_one bias_attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  499  :  32 one_to_one bias_attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  500  :  32 one_to_one bias_attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  501  :  32 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  502  :  32 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  503  :  32 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  504  :  32 one_to_one attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.32224348291257743
running  505  :  32 one_to_one attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  506  :  32 one_to_one attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  507  :  32 one_to_one attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.37355718782791186
running  508  :  32 one_to_one attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3591959344073411
running  509  :  32 one_to_one attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.32051519453531674
running  510  :  32 one_to_one bias_attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  511  :  32 one_to_one bias_attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  512  :  32 one_to_one bias_attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  513  :  32 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  514  :  32 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  515  :  32 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  516  :  32 one_to_one attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4071559368763245
running  517  :  32 one_to_one attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  518  :  32 one_to_one attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  519  :  32 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4643129024957307
running  520  :  32 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.36176779211159804
running  521  :  32 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4016007242351295
running  522  :  32 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  523  :  32 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  524  :  32 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  525  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  526  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  527  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  528  :  32 one_to_one attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.12410756537662285
running  529  :  32 one_to_one attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  530  :  32 one_to_one attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  531  :  32 one_to_one attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.12274962450877518
running  532  :  32 one_to_one attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.12571240458407917
running  533  :  32 one_to_one attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.12141225850256157
running  534  :  32 one_to_one bias_attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  535  :  32 one_to_one bias_attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  536  :  32 one_to_one bias_attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  537  :  32 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  538  :  32 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  539  :  32 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  540  :  32 one_to_one attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6335411394358373
running  541  :  32 one_to_one attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  542  :  32 one_to_one attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  543  :  32 one_to_one attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6621813468304426
running  544  :  32 one_to_one attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6485813632903319
running  545  :  32 one_to_one attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6436433964981586
running  546  :  32 one_to_one bias_attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  547  :  32 one_to_one bias_attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  548  :  32 one_to_one bias_attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  549  :  32 one_to_one bias_attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  550  :  32 one_to_one bias_attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  551  :  32 one_to_one bias_attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  552  :  32 one_to_one attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4691068452564657
running  553  :  32 one_to_one attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  554  :  32 one_to_one attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  555  :  32 one_to_one attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6804929736847519
running  556  :  32 one_to_one attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6623253708618809
running  557  :  32 one_to_one attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5660350184145012
running  558  :  32 one_to_one bias_attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  559  :  32 one_to_one bias_attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  560  :  32 one_to_one bias_attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  561  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  562  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  563  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  564  :  32 one_to_one attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6345081579326379
running  565  :  32 one_to_one attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  566  :  32 one_to_one attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  567  :  32 one_to_one attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6609262802707652
running  568  :  32 one_to_one attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6546509474723783
running  569  :  32 one_to_one attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6455980083533939
running  570  :  32 one_to_one bias_attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  571  :  32 one_to_one bias_attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  572  :  32 one_to_one bias_attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  573  :  32 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  574  :  32 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  575  :  32 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  576  :  32 one_to_one attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.18202580087648912
running  577  :  32 one_to_one attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  578  :  32 one_to_one attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  579  :  32 one_to_one attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6352282780898298
running  580  :  32 one_to_one attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.18930930189494477
running  581  :  32 one_to_one attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1897002242659918
running  582  :  32 one_to_one bias_attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  583  :  32 one_to_one bias_attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  584  :  32 one_to_one bias_attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  585  :  32 one_to_one bias_attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  586  :  32 one_to_one bias_attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  587  :  32 one_to_one bias_attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  588  :  32 one_to_one attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.46227599119395923
running  589  :  32 one_to_one attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  590  :  32 one_to_one attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  591  :  32 one_to_one attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6349402300269531
running  592  :  32 one_to_one attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6264839618953563
running  593  :  32 one_to_one attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4854844351171738
running  594  :  32 one_to_one bias_attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  595  :  32 one_to_one bias_attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  596  :  32 one_to_one bias_attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  597  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  598  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  599  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  600  :  32 one_to_one attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.613048577248318
running  601  :  32 one_to_one attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  602  :  32 one_to_one attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  603  :  32 one_to_one attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6437462708063288
running  604  :  32 one_to_one attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6423060304919449
running  605  :  32 one_to_one attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6312779046560912
running  606  :  32 one_to_one bias_attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  607  :  32 one_to_one bias_attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  608  :  32 one_to_one bias_attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  609  :  32 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  610  :  32 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  611  :  32 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  612  :  32 one_to_one attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1684258173363784
running  613  :  32 one_to_one attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  614  :  32 one_to_one attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  615  :  32 one_to_one attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.15842643458222744
running  616  :  32 one_to_one attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.16350842540583915
running  617  :  32 one_to_one attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.17033927946834557
running  618  :  32 one_to_one bias_attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  619  :  32 one_to_one bias_attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  620  :  32 one_to_one bias_attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  621  :  32 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  622  :  32 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  623  :  32 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  624  :  32 one_to_one attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.2965866304549102
running  625  :  32 one_to_one attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  626  :  32 one_to_one attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  627  :  32 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.16262370635557477
running  628  :  32 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.16266485607884287
running  629  :  32 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1778696788264099
running  630  :  32 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  631  :  32 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  632  :  32 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  633  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  634  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  635  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  636  :  32 one_to_one attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1585704586136658
running  637  :  32 one_to_one attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  638  :  32 one_to_one attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  639  :  32 one_to_one attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.21677674217640885
running  640  :  32 one_to_one attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4391086969940127
running  641  :  32 one_to_one attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1987737382466103
running  642  :  32 one_to_one bias_attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  643  :  32 one_to_one bias_attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  644  :  32 one_to_one bias_attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  645  :  32 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  646  :  32 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  647  :  32 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  648  :  32 one_to_one attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6346316071024423
running  649  :  32 one_to_one attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  650  :  32 one_to_one attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  651  :  32 one_to_one attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6490134353846471
running  652  :  32 one_to_one attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6390346274921301
running  653  :  32 one_to_one attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6589716684155299
running  654  :  32 one_to_one bias_attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  655  :  32 one_to_one bias_attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  656  :  32 one_to_one bias_attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  657  :  32 one_to_one bias_attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  658  :  32 one_to_one bias_attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  659  :  32 one_to_one bias_attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  660  :  32 one_to_one attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.45449869349628624
running  661  :  32 one_to_one attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  662  :  32 one_to_one attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  663  :  32 one_to_one attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4683867250992737
running  664  :  32 one_to_one attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6666255169433986
running  665  :  32 one_to_one attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5250087443161945
running  666  :  32 one_to_one bias_attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  667  :  32 one_to_one bias_attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  668  :  32 one_to_one bias_attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  669  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  670  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  671  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  672  :  32 one_to_one attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6371005904985289
running  673  :  32 one_to_one attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  674  :  32 one_to_one attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  675  :  32 one_to_one attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6407834907310248
running  676  :  32 one_to_one attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6332325165113265
running  677  :  32 one_to_one attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6312984795177252
running  678  :  32 one_to_one bias_attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.70 MiB already allocated; 21.37 GiB free; 520.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  679  :  32 one_to_one bias_attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  680  :  32 one_to_one bias_attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  681  :  32 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.70 MiB already allocated; 21.39 GiB free; 498.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  682  :  32 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.70 MiB already allocated; 21.39 GiB free; 498.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  683  :  32 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.70 MiB already allocated; 21.39 GiB free; 498.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  684  :  32 one_to_one attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.14186367096681274
running  685  :  32 one_to_one attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  686  :  32 one_to_one attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  687  :  32 one_to_one attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6179248194555892
running  688  :  32 one_to_one attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6114848877641298
running  689  :  32 one_to_one attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6097360245252351
running  690  :  32 one_to_one bias_attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  691  :  32 one_to_one bias_attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  692  :  32 one_to_one bias_attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  693  :  32 one_to_one bias_attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  694  :  32 one_to_one bias_attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  695  :  32 one_to_one bias_attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.36 MiB already allocated; 21.41 GiB free; 478.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  696  :  32 one_to_one attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3126350225294735
running  697  :  32 one_to_one attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  698  :  32 one_to_one attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  699  :  32 one_to_one attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5579902475155855
running  700  :  32 one_to_one attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6134806493426331
running  701  :  32 one_to_one attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5944077526078637
running  702  :  32 one_to_one bias_attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  703  :  32 one_to_one bias_attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  704  :  32 one_to_one bias_attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  705  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  706  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  707  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  708  :  32 one_to_one attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6054564533053515
running  709  :  32 one_to_one attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  710  :  32 one_to_one attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  711  :  32 one_to_one attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6126165051540028
running  712  :  32 one_to_one attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6217723185811576
running  713  :  32 one_to_one attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.62193691747423
running  714  :  32 one_to_one bias_attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  715  :  32 one_to_one bias_attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  716  :  32 one_to_one bias_attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  717  :  32 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  718  :  32 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  719  :  32 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  720  :  32 one_to_one attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1330987799107051
running  721  :  32 one_to_one attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  722  :  32 one_to_one attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  723  :  32 one_to_one attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1360615599860091
running  724  :  32 one_to_one attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.13661708125012859
running  725  :  32 one_to_one attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.13766639919346543
running  726  :  32 one_to_one bias_attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  727  :  32 one_to_one bias_attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  728  :  32 one_to_one bias_attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  729  :  32 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  730  :  32 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  731  :  32 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  732  :  32 one_to_one attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.27321358763862313
running  733  :  32 one_to_one attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  734  :  32 one_to_one attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  735  :  32 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.20902001934036993
running  736  :  32 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.13427154702384628
running  737  :  32 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.20576919120218917
running  738  :  32 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  739  :  32 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  740  :  32 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  741  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  742  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  743  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  744  :  32 one_to_one attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3793181490854474
running  745  :  32 one_to_one attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  746  :  32 one_to_one attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  747  :  32 one_to_one attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.15636894841882187
running  748  :  32 one_to_one attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.16243853260086827
running  749  :  32 one_to_one attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.12466308664074234
running  750  :  32 one_to_one bias_attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  751  :  32 one_to_one bias_attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  752  :  32 one_to_one bias_attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  753  :  32 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  754  :  32 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  755  :  32 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  756  :  32 one_to_one attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6140773203300208
running  757  :  32 one_to_one attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  758  :  32 one_to_one attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  759  :  32 one_to_one attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.99 GiB already allocated; 269.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  760  :  32 one_to_one attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.37 GiB already allocated; 19.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  761  :  32 one_to_one attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.20 GiB already allocated; 21.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  762  :  32 one_to_one bias_attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  763  :  32 one_to_one bias_attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  764  :  32 one_to_one bias_attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  765  :  32 one_to_one bias_attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  766  :  32 one_to_one bias_attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  767  :  32 one_to_one bias_attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  768  :  32 one_to_one attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.69 GiB already allocated; 99.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  769  :  32 one_to_one attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  770  :  32 one_to_one attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  771  :  32 one_to_one attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.52 GiB already allocated; 139.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  772  :  32 one_to_one attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 258.00 MiB (GPU 0; 23.70 GiB total capacity; 21.00 GiB already allocated; 147.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  773  :  32 one_to_one attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 208.00 MiB (GPU 0; 23.70 GiB total capacity; 20.91 GiB already allocated; 147.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  774  :  32 one_to_one bias_attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  775  :  32 one_to_one bias_attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  776  :  32 one_to_one bias_attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  777  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  778  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  779  :  32 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  780  :  32 one_to_one attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.66 GiB already allocated; 67.69 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  781  :  32 one_to_one attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  782  :  32 one_to_one attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  783  :  32 one_to_one attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.00 GiB already allocated; 289.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  784  :  32 one_to_one attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.44 GiB already allocated; 123.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  785  :  32 one_to_one attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.49 GiB already allocated; 125.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  786  :  32 one_to_one bias_attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.73 MiB already allocated; 21.42 GiB free; 468.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  787  :  32 one_to_one bias_attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  788  :  32 one_to_one bias_attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  789  :  32 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.73 MiB already allocated; 21.42 GiB free; 468.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  790  :  32 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.73 MiB already allocated; 21.42 GiB free; 468.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  791  :  32 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 451.73 MiB already allocated; 21.42 GiB free; 468.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  792  :  32 one_to_one attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.18529720387630394
running  793  :  32 one_to_one attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  794  :  32 one_to_one attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  795  :  32 one_to_one attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.99 GiB already allocated; 125.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  796  :  32 one_to_one attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.13 GiB already allocated; 123.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  797  :  32 one_to_one attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.14 GiB already allocated; 125.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  798  :  32 one_to_one bias_attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  799  :  32 one_to_one bias_attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  800  :  32 one_to_one bias_attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  801  :  32 one_to_one bias_attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  802  :  32 one_to_one bias_attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  803  :  32 one_to_one bias_attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  804  :  32 one_to_one attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "<string>", line 10, in <backward op>

            def backward(grad_output):
                grad_self = (grad_output * other)._grad_sum_to_size(self_size)
                             ~~~~~~~~~~~~~~~~~~~ <--- HERE
                grad_other = (grad_output * self)._grad_sum_to_size(other_size)
                return grad_self, grad_other
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.28 GiB already allocated; 59.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  805  :  32 one_to_one attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  806  :  32 one_to_one attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  807  :  32 one_to_one attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.19 GiB already allocated; 211.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  808  :  32 one_to_one attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 254.00 MiB (GPU 0; 23.70 GiB total capacity; 20.97 GiB already allocated; 219.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  809  :  32 one_to_one attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.35 GiB already allocated; 7.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  810  :  32 one_to_one bias_attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  811  :  32 one_to_one bias_attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  812  :  32 one_to_one bias_attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  813  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  814  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  815  :  32 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 453.23 MiB already allocated; 21.43 GiB free; 464.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  816  :  32 one_to_one attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.69 GiB already allocated; 61.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  817  :  32 one_to_one attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  818  :  32 one_to_one attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  819  :  32 one_to_one attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.00 GiB already allocated; 289.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  820  :  32 one_to_one attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.46 GiB already allocated; 123.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  821  :  32 one_to_one attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.43 GiB already allocated; 125.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  822  :  32 one_to_one bias_attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  823  :  32 one_to_one bias_attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  824  :  32 one_to_one bias_attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  825  :  32 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  826  :  32 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  827  :  32 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  828  :  32 one_to_one attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.11814085550274675
running  829  :  32 one_to_one attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  830  :  32 one_to_one attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  831  :  32 one_to_one attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.00 GiB already allocated; 89.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  832  :  32 one_to_one attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.30 GiB already allocated; 87.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  833  :  32 one_to_one attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 89.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  834  :  32 one_to_one bias_attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  835  :  32 one_to_one bias_attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  836  :  32 one_to_one bias_attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  837  :  32 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  838  :  32 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  839  :  32 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  840  :  32 one_to_one attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.62 GiB already allocated; 43.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  841  :  32 one_to_one attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  842  :  32 one_to_one attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  843  :  32 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 55.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  844  :  32 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 23.70 GiB total capacity; 21.24 GiB already allocated; 63.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  845  :  32 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 23.70 GiB total capacity; 21.36 GiB already allocated; 59.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  846  :  32 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  847  :  32 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  848  :  32 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  849  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  850  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  851  :  32 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  852  :  32 one_to_one attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3535584223196099
running  853  :  32 one_to_one attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  854  :  32 one_to_one attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  855  :  32 one_to_one attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.00 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  856  :  32 one_to_one attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 23.70 GiB total capacity; 21.38 GiB already allocated; 105.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  857  :  32 one_to_one attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 21.46 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  858  :  32 one_to_one bias_attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  859  :  32 one_to_one bias_attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  860  :  32 one_to_one bias_attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  861  :  32 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  862  :  32 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  863  :  32 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=32, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 452.52 MiB already allocated; 21.43 GiB free; 462.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  864  :  64 one_to_one attention agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6494660823405962
running  865  :  64 one_to_one attention agent_start 0.01 3 0.0 2
skipped because reset not used
running  866  :  64 one_to_one attention agent_start 0.01 3 0.0 3
skipped because reset not used
running  867  :  64 one_to_one attention_reset agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5723515009361562
running  868  :  64 one_to_one attention_reset agent_start 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5567557558175421
running  869  :  64 one_to_one attention_reset agent_start 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.540769088327881
running  870  :  64 one_to_one bias_attention agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.19 MiB already allocated; 21.21 GiB free; 682.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  871  :  64 one_to_one bias_attention agent_start 0.01 3 0.0 2
skipped because reset not used
running  872  :  64 one_to_one bias_attention agent_start 0.01 3 0.0 3
skipped because reset not used
running  873  :  64 one_to_one bias_attention_reset agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.19 MiB already allocated; 21.21 GiB free; 682.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  874  :  64 one_to_one bias_attention_reset agent_start 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.19 MiB already allocated; 21.21 GiB free; 682.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  875  :  64 one_to_one bias_attention_reset agent_start 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.19 MiB already allocated; 21.21 GiB free; 682.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  876  :  64 one_to_one attention last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.30 GiB (GPU 0; 23.70 GiB total capacity; 18.78 GiB already allocated; 1.25 GiB free; 20.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  877  :  64 one_to_one attention last_agent_visited 0.01 3 0.0 2
skipped because reset not used
running  878  :  64 one_to_one attention last_agent_visited 0.01 3 0.0 3
skipped because reset not used
running  879  :  64 one_to_one attention_reset last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.43748328292492233
running  880  :  64 one_to_one attention_reset last_agent_visited 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.571528506470794
running  881  :  64 one_to_one attention_reset last_agent_visited 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 23.70 GiB total capacity; 19.68 GiB already allocated; 287.69 MiB free; 21.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  882  :  64 one_to_one bias_attention last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.08 MiB already allocated; 21.15 GiB free; 752.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  883  :  64 one_to_one bias_attention last_agent_visited 0.01 3 0.0 2
skipped because reset not used
running  884  :  64 one_to_one bias_attention last_agent_visited 0.01 3 0.0 3
skipped because reset not used
running  885  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.08 MiB already allocated; 21.15 GiB free; 752.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  886  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.08 MiB already allocated; 21.15 GiB free; 752.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  887  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.08 MiB already allocated; 21.15 GiB free; 752.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  888  :  64 one_to_one attention node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.647902392856408
running  889  :  64 one_to_one attention node_embedding 0.01 3 0.0 2
skipped because reset not used
running  890  :  64 one_to_one attention node_embedding 0.01 3 0.0 3
skipped because reset not used
running  891  :  64 one_to_one attention_reset node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6531695574347263
running  892  :  64 one_to_one attention_reset node_embedding 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6603296092833776
running  893  :  64 one_to_one attention_reset node_embedding 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6441577680390099
running  894  :  64 one_to_one bias_attention node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  895  :  64 one_to_one bias_attention node_embedding 0.01 3 0.0 2
skipped because reset not used
running  896  :  64 one_to_one bias_attention node_embedding 0.01 3 0.0 3
skipped because reset not used
running  897  :  64 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  898  :  64 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  899  :  64 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  900  :  64 one_to_one attention agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6594654650947472
running  901  :  64 one_to_one attention agent_start 0.001 3 0.0 2
skipped because reset not used
running  902  :  64 one_to_one attention agent_start 0.001 3 0.0 3
skipped because reset not used
running  903  :  64 one_to_one attention_reset agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6606176573462543
running  904  :  64 one_to_one attention_reset agent_start 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6452482357056148
running  905  :  64 one_to_one attention_reset agent_start 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6512972450260271
running  906  :  64 one_to_one bias_attention agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  907  :  64 one_to_one bias_attention agent_start 0.001 3 0.0 2
skipped because reset not used
running  908  :  64 one_to_one bias_attention agent_start 0.001 3 0.0 3
skipped because reset not used
running  909  :  64 one_to_one bias_attention_reset agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  910  :  64 one_to_one bias_attention_reset agent_start 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  911  :  64 one_to_one bias_attention_reset agent_start 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  912  :  64 one_to_one attention last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5711170092381129
running  913  :  64 one_to_one attention last_agent_visited 0.001 3 0.0 2
skipped because reset not used
running  914  :  64 one_to_one attention last_agent_visited 0.001 3 0.0 3
skipped because reset not used
running  915  :  64 one_to_one attention_reset last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6212373721786721
running  916  :  64 one_to_one attention_reset last_agent_visited 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.668271505874123
running  917  :  64 one_to_one attention_reset last_agent_visited 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5906631277904656
running  918  :  64 one_to_one bias_attention last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  919  :  64 one_to_one bias_attention last_agent_visited 0.001 3 0.0 2
skipped because reset not used
running  920  :  64 one_to_one bias_attention last_agent_visited 0.001 3 0.0 3
skipped because reset not used
running  921  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  922  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  923  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  924  :  64 one_to_one attention node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6521819640762916
running  925  :  64 one_to_one attention node_embedding 0.001 3 0.0 2
skipped because reset not used
running  926  :  64 one_to_one attention node_embedding 0.001 3 0.0 3
skipped because reset not used
running  927  :  64 one_to_one attention_reset node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6626339937863918
running  928  :  64 one_to_one attention_reset node_embedding 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6707610641318437
running  929  :  64 one_to_one attention_reset node_embedding 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6527992099253133
running  930  :  64 one_to_one bias_attention node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  931  :  64 one_to_one bias_attention node_embedding 0.001 3 0.0 2
skipped because reset not used
running  932  :  64 one_to_one bias_attention node_embedding 0.001 3 0.0 3
skipped because reset not used
running  933  :  64 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  934  :  64 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  935  :  64 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  936  :  64 one_to_one attention agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6072258914058803
running  937  :  64 one_to_one attention agent_start 0.0001 3 0.0 2
skipped because reset not used
running  938  :  64 one_to_one attention agent_start 0.0001 3 0.0 3
skipped because reset not used
running  939  :  64 one_to_one attention_reset agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6192416106001687
running  940  :  64 one_to_one attention_reset agent_start 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.610476719544061
running  941  :  64 one_to_one attention_reset agent_start 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6073904902989528
running  942  :  64 one_to_one bias_attention agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  943  :  64 one_to_one bias_attention agent_start 0.0001 3 0.0 2
skipped because reset not used
running  944  :  64 one_to_one bias_attention agent_start 0.0001 3 0.0 3
skipped because reset not used
running  945  :  64 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  946  :  64 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  947  :  64 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  948  :  64 one_to_one attention last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.48433224286566673
running  949  :  64 one_to_one attention last_agent_visited 0.0001 3 0.0 2
skipped because reset not used
running  950  :  64 one_to_one attention last_agent_visited 0.0001 3 0.0 3
skipped because reset not used
running  951  :  64 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.559677386169578
running  952  :  64 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6085426825504598
running  953  :  64 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.4793531263502253
running  954  :  64 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  955  :  64 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 2
skipped because reset not used
running  956  :  64 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 3
skipped because reset not used
running  957  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  958  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  959  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  960  :  64 one_to_one attention node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6031932185256054
running  961  :  64 one_to_one attention node_embedding 0.0001 3 0.0 2
skipped because reset not used
running  962  :  64 one_to_one attention node_embedding 0.0001 3 0.0 3
skipped because reset not used
running  963  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6082546344875831
running  964  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6174721724996399
running  965  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.602699421846388
running  966  :  64 one_to_one bias_attention node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  967  :  64 one_to_one bias_attention node_embedding 0.0001 3 0.0 2
skipped because reset not used
running  968  :  64 one_to_one bias_attention node_embedding 0.0001 3 0.0 3
skipped because reset not used
running  969  :  64 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  970  :  64 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  971  :  64 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  972  :  64 one_to_one attention agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5454395819188116
running  973  :  64 one_to_one attention agent_start 0.01 6 0.0 2
skipped because reset not used
running  974  :  64 one_to_one attention agent_start 0.01 6 0.0 3
skipped because reset not used
running  975  :  64 one_to_one attention_reset agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 23.70 GiB total capacity; 19.17 GiB already allocated; 477.69 MiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  976  :  64 one_to_one attention_reset agent_start 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.11 GiB already allocated; 101.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  977  :  64 one_to_one attention_reset agent_start 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.33 GiB already allocated; 147.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  978  :  64 one_to_one bias_attention agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  979  :  64 one_to_one bias_attention agent_start 0.01 6 0.0 2
skipped because reset not used
running  980  :  64 one_to_one bias_attention agent_start 0.01 6 0.0 3
skipped because reset not used
running  981  :  64 one_to_one bias_attention_reset agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  982  :  64 one_to_one bias_attention_reset agent_start 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  983  :  64 one_to_one bias_attention_reset agent_start 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  984  :  64 one_to_one attention last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 358.00 MiB (GPU 0; 23.70 GiB total capacity; 20.05 GiB already allocated; 233.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  985  :  64 one_to_one attention last_agent_visited 0.01 6 0.0 2
skipped because reset not used
running  986  :  64 one_to_one attention last_agent_visited 0.01 6 0.0 3
skipped because reset not used
running  987  :  64 one_to_one attention_reset last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 23.70 GiB total capacity; 20.09 GiB already allocated; 353.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  988  :  64 one_to_one attention_reset last_agent_visited 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 446.00 MiB (GPU 0; 23.70 GiB total capacity; 20.45 GiB already allocated; 353.69 MiB free; 21.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  989  :  64 one_to_one attention_reset last_agent_visited 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.84 GiB already allocated; 139.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  990  :  64 one_to_one bias_attention last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  991  :  64 one_to_one bias_attention last_agent_visited 0.01 6 0.0 2
skipped because reset not used
running  992  :  64 one_to_one bias_attention last_agent_visited 0.01 6 0.0 3
skipped because reset not used
running  993  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  994  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  995  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  996  :  64 one_to_one attention node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.91 GiB already allocated; 167.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  997  :  64 one_to_one attention node_embedding 0.01 6 0.0 2
skipped because reset not used
running  998  :  64 one_to_one attention node_embedding 0.01 6 0.0 3
skipped because reset not used
running  999  :  64 one_to_one attention_reset node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 20.86 GiB already allocated; 139.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1000  :  64 one_to_one attention_reset node_embedding 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.11 GiB already allocated; 141.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1001  :  64 one_to_one attention_reset node_embedding 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.33 GiB already allocated; 147.69 MiB free; 21.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1002  :  64 one_to_one bias_attention node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1003  :  64 one_to_one bias_attention node_embedding 0.01 6 0.0 2
skipped because reset not used
running  1004  :  64 one_to_one bias_attention node_embedding 0.01 6 0.0 3
skipped because reset not used
running  1005  :  64 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1006  :  64 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1007  :  64 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1008  :  64 one_to_one attention agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 346.00 MiB (GPU 0; 23.70 GiB total capacity; 19.76 GiB already allocated; 17.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1009  :  64 one_to_one attention agent_start 0.001 6 0.0 2
skipped because reset not used
running  1010  :  64 one_to_one attention agent_start 0.001 6 0.0 3
skipped because reset not used
running  1011  :  64 one_to_one attention_reset agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 23.70 GiB total capacity; 18.45 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1012  :  64 one_to_one attention_reset agent_start 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.28 GiB already allocated; 15.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1013  :  64 one_to_one attention_reset agent_start 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.33 GiB already allocated; 19.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1014  :  64 one_to_one bias_attention agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1015  :  64 one_to_one bias_attention agent_start 0.001 6 0.0 2
skipped because reset not used
running  1016  :  64 one_to_one bias_attention agent_start 0.001 6 0.0 3
skipped because reset not used
running  1017  :  64 one_to_one bias_attention_reset agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1018  :  64 one_to_one bias_attention_reset agent_start 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1019  :  64 one_to_one bias_attention_reset agent_start 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1020  :  64 one_to_one attention last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.49 GiB already allocated; 77.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1021  :  64 one_to_one attention last_agent_visited 0.001 6 0.0 2
skipped because reset not used
running  1022  :  64 one_to_one attention last_agent_visited 0.001 6 0.0 3
skipped because reset not used
running  1023  :  64 one_to_one attention_reset last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.33 GiB already allocated; 41.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1024  :  64 one_to_one attention_reset last_agent_visited 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 446.00 MiB (GPU 0; 23.70 GiB total capacity; 20.44 GiB already allocated; 47.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1025  :  64 one_to_one attention_reset last_agent_visited 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 47.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1026  :  64 one_to_one bias_attention last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1027  :  64 one_to_one bias_attention last_agent_visited 0.001 6 0.0 2
skipped because reset not used
running  1028  :  64 one_to_one bias_attention last_agent_visited 0.001 6 0.0 3
skipped because reset not used
running  1029  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1030  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1031  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1032  :  64 one_to_one attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6447132893031294
running  1033  :  64 one_to_one attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  1034  :  64 one_to_one attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  1035  :  64 one_to_one attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 612.00 MiB (GPU 0; 23.70 GiB total capacity; 19.26 GiB already allocated; 129.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1036  :  64 one_to_one attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.12 GiB already allocated; 123.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1037  :  64 one_to_one attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 23.70 GiB total capacity; 20.46 GiB already allocated; 129.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1038  :  64 one_to_one bias_attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1039  :  64 one_to_one bias_attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  1040  :  64 one_to_one bias_attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  1041  :  64 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1042  :  64 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1043  :  64 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1044  :  64 one_to_one attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 348.00 MiB (GPU 0; 23.70 GiB total capacity; 19.81 GiB already allocated; 251.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1045  :  64 one_to_one attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  1046  :  64 one_to_one attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  1047  :  64 one_to_one attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 544.00 MiB (GPU 0; 23.70 GiB total capacity; 19.15 GiB already allocated; 209.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1048  :  64 one_to_one attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.28 GiB already allocated; 39.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1049  :  64 one_to_one attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.33 GiB already allocated; 453.69 MiB free; 21.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1050  :  64 one_to_one bias_attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1051  :  64 one_to_one bias_attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  1052  :  64 one_to_one bias_attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  1053  :  64 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1054  :  64 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1055  :  64 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1056  :  64 one_to_one attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.34 GiB already allocated; 209.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1057  :  64 one_to_one attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  1058  :  64 one_to_one attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  1059  :  64 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.33 GiB already allocated; 7.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1060  :  64 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 446.00 MiB (GPU 0; 23.70 GiB total capacity; 20.88 GiB already allocated; 13.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1061  :  64 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.84 GiB already allocated; 13.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1062  :  64 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1063  :  64 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  1064  :  64 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  1065  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1066  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1067  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1068  :  64 one_to_one attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.580787194206119
running  1069  :  64 one_to_one attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  1070  :  64 one_to_one attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  1071  :  64 one_to_one attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 23.70 GiB total capacity; 19.18 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1072  :  64 one_to_one attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.12 GiB already allocated; 21.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1073  :  64 one_to_one attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 23.70 GiB total capacity; 20.34 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1074  :  64 one_to_one bias_attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1075  :  64 one_to_one bias_attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  1076  :  64 one_to_one bias_attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  1077  :  64 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1078  :  64 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1079  :  64 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1080  :  64 one_to_one attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.50 GiB already allocated; 119.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1081  :  64 one_to_one attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  1082  :  64 one_to_one attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  1083  :  64 one_to_one attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 105.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1084  :  64 one_to_one attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.68 GiB already allocated; 109.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1085  :  64 one_to_one attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 23.70 GiB total capacity; 20.16 GiB already allocated; 441.69 MiB free; 21.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1086  :  64 one_to_one bias_attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1087  :  64 one_to_one bias_attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  1088  :  64 one_to_one bias_attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  1089  :  64 one_to_one bias_attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1090  :  64 one_to_one bias_attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1091  :  64 one_to_one bias_attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1092  :  64 one_to_one attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.35 GiB already allocated; 153.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1093  :  64 one_to_one attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  1094  :  64 one_to_one attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  1095  :  64 one_to_one attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.21 GiB already allocated; 57.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1096  :  64 one_to_one attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 63.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1097  :  64 one_to_one attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 63.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1098  :  64 one_to_one bias_attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1099  :  64 one_to_one bias_attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  1100  :  64 one_to_one bias_attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  1101  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1102  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1103  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1104  :  64 one_to_one attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 137.69 MiB free; 21.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1105  :  64 one_to_one attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  1106  :  64 one_to_one attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  1107  :  64 one_to_one attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1108  :  64 one_to_one attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.68 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1109  :  64 one_to_one attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 220.00 MiB (GPU 0; 23.70 GiB total capacity; 21.47 GiB already allocated; 25.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1110  :  64 one_to_one bias_attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1111  :  64 one_to_one bias_attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  1112  :  64 one_to_one bias_attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  1113  :  64 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1114  :  64 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1115  :  64 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1116  :  64 one_to_one attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.50 GiB already allocated; 121.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1117  :  64 one_to_one attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  1118  :  64 one_to_one attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  1119  :  64 one_to_one attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1120  :  64 one_to_one attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.68 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1121  :  64 one_to_one attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 220.00 MiB (GPU 0; 23.70 GiB total capacity; 21.47 GiB already allocated; 25.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1122  :  64 one_to_one bias_attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1123  :  64 one_to_one bias_attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  1124  :  64 one_to_one bias_attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  1125  :  64 one_to_one bias_attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1126  :  64 one_to_one bias_attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1127  :  64 one_to_one bias_attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1128  :  64 one_to_one attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.68 GiB already allocated; 5.69 MiB free; 21.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1129  :  64 one_to_one attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  1130  :  64 one_to_one attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  1131  :  64 one_to_one attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.13 GiB already allocated; 21.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1132  :  64 one_to_one attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 422.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1133  :  64 one_to_one attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 23.70 GiB total capacity; 21.17 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1134  :  64 one_to_one bias_attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1135  :  64 one_to_one bias_attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  1136  :  64 one_to_one bias_attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  1137  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1138  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1139  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1140  :  64 one_to_one attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 99.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1141  :  64 one_to_one attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  1142  :  64 one_to_one attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  1143  :  64 one_to_one attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 85.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1144  :  64 one_to_one attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.68 GiB already allocated; 89.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1145  :  64 one_to_one attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 23.70 GiB total capacity; 20.15 GiB already allocated; 421.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1146  :  64 one_to_one bias_attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1147  :  64 one_to_one bias_attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  1148  :  64 one_to_one bias_attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  1149  :  64 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1150  :  64 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1151  :  64 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1152  :  64 one_to_one attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.51 GiB already allocated; 121.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1153  :  64 one_to_one attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  1154  :  64 one_to_one attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  1155  :  64 one_to_one attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 23.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1156  :  64 one_to_one attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.67 GiB already allocated; 27.69 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1157  :  64 one_to_one attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 220.00 MiB (GPU 0; 23.70 GiB total capacity; 21.47 GiB already allocated; 25.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1158  :  64 one_to_one bias_attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1159  :  64 one_to_one bias_attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  1160  :  64 one_to_one bias_attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  1161  :  64 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1162  :  64 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1163  :  64 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1164  :  64 one_to_one attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.70 GiB total capacity; 21.36 GiB already allocated; 153.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1165  :  64 one_to_one attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  1166  :  64 one_to_one attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  1167  :  64 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 23.70 GiB total capacity; 21.22 GiB already allocated; 57.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1168  :  64 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 420.00 MiB (GPU 0; 23.70 GiB total capacity; 20.48 GiB already allocated; 63.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1169  :  64 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 592.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 63.69 MiB free; 21.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1170  :  64 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1171  :  64 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  1172  :  64 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  1173  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1174  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1175  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1176  :  64 one_to_one attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.50 GiB already allocated; 121.69 MiB free; 21.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1177  :  64 one_to_one attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  1178  :  64 one_to_one attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  1179  :  64 one_to_one attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 13.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1180  :  64 one_to_one attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.67 GiB already allocated; 17.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1181  :  64 one_to_one attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 218.00 MiB (GPU 0; 23.70 GiB total capacity; 21.47 GiB already allocated; 15.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1182  :  64 one_to_one bias_attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1183  :  64 one_to_one bias_attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  1184  :  64 one_to_one bias_attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  1185  :  64 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1186  :  64 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1187  :  64 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1188  :  64 one_to_one attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.48 GiB already allocated; 149.69 MiB free; 21.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1189  :  64 one_to_one attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  1190  :  64 one_to_one attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  1191  :  64 one_to_one attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.96 GiB already allocated; 325.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1192  :  64 one_to_one attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 23.70 GiB total capacity; 18.96 GiB already allocated; 333.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1193  :  64 one_to_one attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 111.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1194  :  64 one_to_one bias_attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1195  :  64 one_to_one bias_attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  1196  :  64 one_to_one bias_attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  1197  :  64 one_to_one bias_attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1198  :  64 one_to_one bias_attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1199  :  64 one_to_one bias_attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1200  :  64 one_to_one attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.39 GiB already allocated; 219.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1201  :  64 one_to_one attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  1202  :  64 one_to_one attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  1203  :  64 one_to_one attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.77 GiB already allocated; 239.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1204  :  64 one_to_one attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.70 GiB total capacity; 19.96 GiB already allocated; 243.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1205  :  64 one_to_one attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 550.00 MiB (GPU 0; 23.70 GiB total capacity; 20.62 GiB already allocated; 243.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1206  :  64 one_to_one bias_attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1207  :  64 one_to_one bias_attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  1208  :  64 one_to_one bias_attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  1209  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1210  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1211  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1212  :  64 one_to_one attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.48 GiB already allocated; 179.69 MiB free; 21.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1213  :  64 one_to_one attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  1214  :  64 one_to_one attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  1215  :  64 one_to_one attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 7.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1216  :  64 one_to_one attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 438.00 MiB (GPU 0; 23.70 GiB total capacity; 18.95 GiB already allocated; 15.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1217  :  64 one_to_one attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 434.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 9.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1218  :  64 one_to_one bias_attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1219  :  64 one_to_one bias_attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  1220  :  64 one_to_one bias_attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  1221  :  64 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1222  :  64 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1223  :  64 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1224  :  64 one_to_one attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.47 GiB already allocated; 209.69 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1225  :  64 one_to_one attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  1226  :  64 one_to_one attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  1227  :  64 one_to_one attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.96 GiB already allocated; 255.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1228  :  64 one_to_one attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 23.70 GiB total capacity; 19.39 GiB already allocated; 263.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1229  :  64 one_to_one attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 41.69 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1230  :  64 one_to_one bias_attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1231  :  64 one_to_one bias_attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  1232  :  64 one_to_one bias_attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  1233  :  64 one_to_one bias_attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1234  :  64 one_to_one bias_attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1235  :  64 one_to_one bias_attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1236  :  64 one_to_one attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.41 GiB already allocated; 219.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1237  :  64 one_to_one attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  1238  :  64 one_to_one attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  1239  :  64 one_to_one attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.77 GiB already allocated; 239.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1240  :  64 one_to_one attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.70 GiB total capacity; 19.96 GiB already allocated; 243.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1241  :  64 one_to_one attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 548.00 MiB (GPU 0; 23.70 GiB total capacity; 20.62 GiB already allocated; 243.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1242  :  64 one_to_one bias_attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1243  :  64 one_to_one bias_attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  1244  :  64 one_to_one bias_attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  1245  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1246  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1247  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1248  :  64 one_to_one attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.48 GiB already allocated; 159.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1249  :  64 one_to_one attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  1250  :  64 one_to_one attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  1251  :  64 one_to_one attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.96 GiB already allocated; 325.69 MiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1252  :  64 one_to_one attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.70 GiB total capacity; 18.95 GiB already allocated; 333.69 MiB free; 21.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1253  :  64 one_to_one attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 111.69 MiB free; 21.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1254  :  64 one_to_one bias_attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1255  :  64 one_to_one bias_attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  1256  :  64 one_to_one bias_attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  1257  :  64 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1258  :  64 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1259  :  64 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1260  :  64 one_to_one attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.49 GiB already allocated; 159.69 MiB free; 21.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1261  :  64 one_to_one attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  1262  :  64 one_to_one attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  1263  :  64 one_to_one attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 7.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1264  :  64 one_to_one attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 23.70 GiB total capacity; 18.96 GiB already allocated; 15.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1265  :  64 one_to_one attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 434.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 9.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1266  :  64 one_to_one bias_attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1267  :  64 one_to_one bias_attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  1268  :  64 one_to_one bias_attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  1269  :  64 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1270  :  64 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1271  :  64 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1272  :  64 one_to_one attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.39 GiB already allocated; 219.69 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1273  :  64 one_to_one attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  1274  :  64 one_to_one attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  1275  :  64 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 20.77 GiB already allocated; 249.69 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1276  :  64 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 23.70 GiB total capacity; 19.97 GiB already allocated; 253.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1277  :  64 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 548.00 MiB (GPU 0; 23.70 GiB total capacity; 20.62 GiB already allocated; 253.69 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1278  :  64 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1279  :  64 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  1280  :  64 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  1281  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1282  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1283  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1284  :  64 one_to_one attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.70 GiB total capacity; 21.48 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1285  :  64 one_to_one attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  1286  :  64 one_to_one attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  1287  :  64 one_to_one attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.70 GiB total capacity; 20.96 GiB already allocated; 231.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1288  :  64 one_to_one attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 23.70 GiB total capacity; 19.39 GiB already allocated; 239.69 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1289  :  64 one_to_one attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 434.00 MiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 15.69 MiB free; 21.87 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1290  :  64 one_to_one bias_attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1291  :  64 one_to_one bias_attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  1292  :  64 one_to_one bias_attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  1293  :  64 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1294  :  64 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1295  :  64 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.11 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1296  :  64 one_to_one attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6670987387609818
running  1297  :  64 one_to_one attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  1298  :  64 one_to_one attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  1299  :  64 one_to_one attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.682344711231817
running  1300  :  64 one_to_one attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6783531880748102
running  1301  :  64 one_to_one attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6770569717918647
running  1302  :  64 one_to_one bias_attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1303  :  64 one_to_one bias_attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  1304  :  64 one_to_one bias_attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  1305  :  64 one_to_one bias_attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1306  :  64 one_to_one bias_attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1307  :  64 one_to_one bias_attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1308  :  64 one_to_one attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.47610229821204453
running  1309  :  64 one_to_one attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  1310  :  64 one_to_one attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  1311  :  64 one_to_one attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5382383803468922
running  1312  :  64 one_to_one attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6859864617410448
running  1313  :  64 one_to_one attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.3842561158776207
running  1314  :  64 one_to_one bias_attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1315  :  64 one_to_one bias_attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  1316  :  64 one_to_one bias_attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  1317  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1318  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1319  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1320  :  64 one_to_one attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6532518568812625
running  1321  :  64 one_to_one attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  1322  :  64 one_to_one attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  1323  :  64 one_to_one attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.659362590786577
running  1324  :  64 one_to_one attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6673867868238587
running  1325  :  64 one_to_one attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6479229677180421
running  1326  :  64 one_to_one bias_attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1327  :  64 one_to_one bias_attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  1328  :  64 one_to_one bias_attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  1329  :  64 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1330  :  64 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1331  :  64 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1332  :  64 one_to_one attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6537868032837479
running  1333  :  64 one_to_one attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  1334  :  64 one_to_one attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  1335  :  64 one_to_one attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6765631751126474
running  1336  :  64 one_to_one attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6764397259428431
running  1337  :  64 one_to_one attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6539925519000884
running  1338  :  64 one_to_one bias_attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1339  :  64 one_to_one bias_attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  1340  :  64 one_to_one bias_attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  1341  :  64 one_to_one bias_attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1342  :  64 one_to_one bias_attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1343  :  64 one_to_one bias_attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1344  :  64 one_to_one attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5352344505483201
running  1345  :  64 one_to_one attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  1346  :  64 one_to_one attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  1347  :  64 one_to_one attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6097771742485032
running  1348  :  64 one_to_one attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6784560623829805
running  1349  :  64 one_to_one attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5605209554965743
running  1350  :  64 one_to_one bias_attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1351  :  64 one_to_one bias_attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  1352  :  64 one_to_one bias_attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  1353  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1354  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1355  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1356  :  64 one_to_one attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6482110157809189
running  1357  :  64 one_to_one attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  1358  :  64 one_to_one attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  1359  :  64 one_to_one attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6655967738616958
running  1360  :  64 one_to_one attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6678394337798078
running  1361  :  64 one_to_one attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6491368845544514
running  1362  :  64 one_to_one bias_attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1363  :  64 one_to_one bias_attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  1364  :  64 one_to_one bias_attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  1365  :  64 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1366  :  64 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1367  :  64 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1368  :  64 one_to_one attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5095570232290187
running  1369  :  64 one_to_one attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  1370  :  64 one_to_one attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  1371  :  64 one_to_one attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5519412381951732
running  1372  :  64 one_to_one attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.543731868403185
running  1373  :  64 one_to_one attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5123552044112504
running  1374  :  64 one_to_one bias_attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1375  :  64 one_to_one bias_attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  1376  :  64 one_to_one bias_attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  1377  :  64 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1378  :  64 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1379  :  64 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1380  :  64 one_to_one attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.515976380058844
running  1381  :  64 one_to_one attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  1382  :  64 one_to_one attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  1383  :  64 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5616731477480814
running  1384  :  64 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.542229903503899
running  1385  :  64 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5199884780774849
running  1386  :  64 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1387  :  64 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  1388  :  64 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  1389  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1390  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1391  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 598.10 MiB already allocated; 21.23 GiB free; 670.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1392  :  64 one_to_one attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5676604324835915
running  1393  :  64 one_to_one attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  1394  :  64 one_to_one attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  1395  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5965063884945374
running  1396  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6016706787646853
running  1397  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
