Running on host: tikgpu04
In directory: /home/janulm/janulm_agent_nodes/agent-net
Starting on: Mon Jan 2 11:52:38 CET 2023
SLURM_JOB_ID: 583482
running  1397  :  64 one_to_one attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.5673106598358126
running  1398  :  64 one_to_one bias_attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 598.40 MiB already allocated; 21.91 GiB free; 616.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1399  :  64 one_to_one bias_attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  1400  :  64 one_to_one bias_attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  1401  :  64 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.11 MiB already allocated; 21.91 GiB free; 618.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1402  :  64 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.44 MiB already allocated; 21.88 GiB free; 640.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1403  :  64 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.44 MiB already allocated; 21.88 GiB free; 640.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1404  :  64 one_to_one attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 415.44 MiB free; 22.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1405  :  64 one_to_one attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  1406  :  64 one_to_one attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  1407  :  64 one_to_one attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 21.62 GiB already allocated; 77.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1408  :  64 one_to_one attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 87.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1409  :  64 one_to_one attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 87.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1410  :  64 one_to_one bias_attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.44 MiB already allocated; 21.88 GiB free; 640.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1411  :  64 one_to_one bias_attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  1412  :  64 one_to_one bias_attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  1413  :  64 one_to_one bias_attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.44 MiB already allocated; 21.88 GiB free; 640.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1414  :  64 one_to_one bias_attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.44 MiB already allocated; 21.88 GiB free; 640.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1415  :  64 one_to_one bias_attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.44 MiB already allocated; 21.88 GiB free; 640.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1416  :  64 one_to_one attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 368.00 MiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 151.44 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1417  :  64 one_to_one attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  1418  :  64 one_to_one attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  1419  :  64 one_to_one attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.74 GiB already allocated; 279.44 MiB free; 22.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1420  :  64 one_to_one attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 21.34 GiB already allocated; 283.44 MiB free; 22.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1421  :  64 one_to_one attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 20.61 GiB already allocated; 7.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1422  :  64 one_to_one bias_attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1423  :  64 one_to_one bias_attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  1424  :  64 one_to_one bias_attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  1425  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1426  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1427  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1428  :  64 one_to_one attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 496.00 MiB (GPU 0; 23.65 GiB total capacity; 21.62 GiB already allocated; 457.44 MiB free; 22.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1429  :  64 one_to_one attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  1430  :  64 one_to_one attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  1431  :  64 one_to_one attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 21.36 GiB already allocated; 119.44 MiB free; 22.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1432  :  64 one_to_one attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 131.44 MiB free; 22.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1433  :  64 one_to_one attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 20.77 GiB already allocated; 131.44 MiB free; 22.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1434  :  64 one_to_one bias_attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1435  :  64 one_to_one bias_attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  1436  :  64 one_to_one bias_attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  1437  :  64 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1438  :  64 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1439  :  64 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1440  :  64 one_to_one attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6377589860708187
running  1441  :  64 one_to_one attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  1442  :  64 one_to_one attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  1443  :  64 one_to_one attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.67160463345884
running  1444  :  64 one_to_one attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.62 GiB already allocated; 177.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1445  :  64 one_to_one attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 21.30 GiB already allocated; 221.44 MiB free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1446  :  64 one_to_one bias_attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1447  :  64 one_to_one bias_attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  1448  :  64 one_to_one bias_attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  1449  :  64 one_to_one bias_attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1450  :  64 one_to_one bias_attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1451  :  64 one_to_one bias_attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1452  :  64 one_to_one attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.47225479908647616
running  1453  :  64 one_to_one attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  1454  :  64 one_to_one attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  1455  :  64 one_to_one attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 49, in spmm
    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)

    out = matrix.index_select(-2, col)
          ~~~~~~~~~~~~~~~~~~~ <--- HERE
    out = out * value.unsqueeze(-1)
    if reduce == 'sum':
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

running  1456  :  64 one_to_one attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 21.34 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1457  :  64 one_to_one attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 21.20 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1458  :  64 one_to_one bias_attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1459  :  64 one_to_one bias_attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  1460  :  64 one_to_one bias_attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  1461  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1462  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1463  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1464  :  64 one_to_one attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.6466884760199988
running  1465  :  64 one_to_one attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  1466  :  64 one_to_one attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  1467  :  64 one_to_one attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 546.00 MiB (GPU 0; 23.65 GiB total capacity; 19.55 GiB already allocated; 205.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1468  :  64 one_to_one attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 207.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1469  :  64 one_to_one attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 207.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1470  :  64 one_to_one bias_attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1471  :  64 one_to_one bias_attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  1472  :  64 one_to_one bias_attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  1473  :  64 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1474  :  64 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1475  :  64 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1476  :  64 one_to_one attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.1849885809517931
running  1477  :  64 one_to_one attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  1478  :  64 one_to_one attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  1479  :  64 one_to_one attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 606.00 MiB (GPU 0; 23.65 GiB total capacity; 19.95 GiB already allocated; 213.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1480  :  64 one_to_one attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 215.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1481  :  64 one_to_one attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 21.30 GiB already allocated; 255.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1482  :  64 one_to_one bias_attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1483  :  64 one_to_one bias_attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  1484  :  64 one_to_one bias_attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  1485  :  64 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1486  :  64 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1487  :  64 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1488  :  64 one_to_one attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.40645639158076663
running  1489  :  64 one_to_one attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  1490  :  64 one_to_one attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  1491  :  64 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.74 GiB already allocated; 153.44 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1492  :  64 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 21.34 GiB already allocated; 157.44 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1493  :  64 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 21.20 GiB already allocated; 157.44 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1494  :  64 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1495  :  64 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  1496  :  64 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  1497  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1498  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1499  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1500  :  64 one_to_one attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
test_acc:  0.16859041622945087
running  1501  :  64 one_to_one attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  1502  :  64 one_to_one attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  1503  :  64 one_to_one attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 19.94 GiB already allocated; 311.44 MiB free; 22.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1504  :  64 one_to_one attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 462.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 313.44 MiB free; 22.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1505  :  64 one_to_one attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 604.00 MiB (GPU 0; 23.65 GiB total capacity; 20.78 GiB already allocated; 313.44 MiB free; 22.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1506  :  64 one_to_one bias_attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1507  :  64 one_to_one bias_attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  1508  :  64 one_to_one bias_attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  1509  :  64 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1510  :  64 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1511  :  64 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1512  :  64 one_to_one attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 23.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1513  :  64 one_to_one attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  1514  :  64 one_to_one attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  1515  :  64 one_to_one attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.98 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1516  :  64 one_to_one attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1517  :  64 one_to_one attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.65 GiB total capacity; 21.95 GiB already allocated; 109.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1518  :  64 one_to_one bias_attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1519  :  64 one_to_one bias_attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  1520  :  64 one_to_one bias_attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  1521  :  64 one_to_one bias_attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1522  :  64 one_to_one bias_attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1523  :  64 one_to_one bias_attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1524  :  64 one_to_one attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.23 GiB already allocated; 71.44 MiB free; 22.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1525  :  64 one_to_one attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  1526  :  64 one_to_one attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  1527  :  64 one_to_one attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.70 GiB already allocated; 21.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1528  :  64 one_to_one attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 452.00 MiB (GPU 0; 23.65 GiB total capacity; 21.30 GiB already allocated; 25.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1529  :  64 one_to_one attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 596.00 MiB (GPU 0; 23.65 GiB total capacity; 21.77 GiB already allocated; 25.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1530  :  64 one_to_one bias_attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1531  :  64 one_to_one bias_attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  1532  :  64 one_to_one bias_attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  1533  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1534  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1535  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1536  :  64 one_to_one attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 23.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1537  :  64 one_to_one attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  1538  :  64 one_to_one attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  1539  :  64 one_to_one attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.98 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1540  :  64 one_to_one attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1541  :  64 one_to_one attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.65 GiB total capacity; 21.95 GiB already allocated; 109.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1542  :  64 one_to_one bias_attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1543  :  64 one_to_one bias_attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  1544  :  64 one_to_one bias_attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  1545  :  64 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1546  :  64 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1547  :  64 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1548  :  64 one_to_one attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 23.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1549  :  64 one_to_one attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  1550  :  64 one_to_one attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  1551  :  64 one_to_one attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.98 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1552  :  64 one_to_one attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1553  :  64 one_to_one attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.65 GiB total capacity; 21.95 GiB already allocated; 109.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1554  :  64 one_to_one bias_attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1555  :  64 one_to_one bias_attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  1556  :  64 one_to_one bias_attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  1557  :  64 one_to_one bias_attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1558  :  64 one_to_one bias_attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1559  :  64 one_to_one bias_attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1560  :  64 one_to_one attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.23 GiB already allocated; 71.44 MiB free; 22.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1561  :  64 one_to_one attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  1562  :  64 one_to_one attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  1563  :  64 one_to_one attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.70 GiB already allocated; 21.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1564  :  64 one_to_one attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 452.00 MiB (GPU 0; 23.65 GiB total capacity; 21.30 GiB already allocated; 25.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1565  :  64 one_to_one attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 596.00 MiB (GPU 0; 23.65 GiB total capacity; 21.77 GiB already allocated; 25.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1566  :  64 one_to_one bias_attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1567  :  64 one_to_one bias_attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  1568  :  64 one_to_one bias_attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  1569  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1570  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1571  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1572  :  64 one_to_one attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 23.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1573  :  64 one_to_one attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  1574  :  64 one_to_one attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  1575  :  64 one_to_one attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.98 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1576  :  64 one_to_one attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1577  :  64 one_to_one attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.65 GiB total capacity; 21.95 GiB already allocated; 109.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1578  :  64 one_to_one bias_attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1579  :  64 one_to_one bias_attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  1580  :  64 one_to_one bias_attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  1581  :  64 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1582  :  64 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1583  :  64 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1584  :  64 one_to_one attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 23.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1585  :  64 one_to_one attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  1586  :  64 one_to_one attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  1587  :  64 one_to_one attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.98 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1588  :  64 one_to_one attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1589  :  64 one_to_one attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.65 GiB total capacity; 21.95 GiB already allocated; 109.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1590  :  64 one_to_one bias_attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1591  :  64 one_to_one bias_attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  1592  :  64 one_to_one bias_attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  1593  :  64 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1594  :  64 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1595  :  64 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1596  :  64 one_to_one attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.23 GiB already allocated; 71.44 MiB free; 22.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1597  :  64 one_to_one attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  1598  :  64 one_to_one attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  1599  :  64 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.70 GiB already allocated; 21.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1600  :  64 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 452.00 MiB (GPU 0; 23.65 GiB total capacity; 21.30 GiB already allocated; 25.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1601  :  64 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 596.00 MiB (GPU 0; 23.65 GiB total capacity; 21.77 GiB already allocated; 25.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1602  :  64 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1603  :  64 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  1604  :  64 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  1605  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1606  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1607  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1608  :  64 one_to_one attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 22.29 GiB already allocated; 23.44 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1609  :  64 one_to_one attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  1610  :  64 one_to_one attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  1611  :  64 one_to_one attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.98 GiB already allocated; 107.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1612  :  64 one_to_one attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.57 GiB already allocated; 111.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1613  :  64 one_to_one attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 228.00 MiB (GPU 0; 23.65 GiB total capacity; 21.95 GiB already allocated; 109.44 MiB free; 22.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1614  :  64 one_to_one bias_attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1615  :  64 one_to_one bias_attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  1616  :  64 one_to_one bias_attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  1617  :  64 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1618  :  64 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1619  :  64 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.48 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1620  :  64 one_to_one attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 23.65 GiB total capacity; 22.15 GiB already allocated; 93.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1621  :  64 one_to_one attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  1622  :  64 one_to_one attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  1623  :  64 one_to_one attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.92 GiB already allocated; 213.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1624  :  64 one_to_one attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.04 GiB already allocated; 47.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1625  :  64 one_to_one attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.65 GiB total capacity; 21.87 GiB already allocated; 49.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1626  :  64 one_to_one bias_attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1627  :  64 one_to_one bias_attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  1628  :  64 one_to_one bias_attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  1629  :  64 one_to_one bias_attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1630  :  64 one_to_one bias_attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1631  :  64 one_to_one bias_attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1632  :  64 one_to_one attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 23.65 GiB total capacity; 22.19 GiB already allocated; 87.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1633  :  64 one_to_one attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  1634  :  64 one_to_one attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  1635  :  64 one_to_one attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 7.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1636  :  64 one_to_one attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 23.65 GiB total capacity; 21.55 GiB already allocated; 11.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1637  :  64 one_to_one attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 556.00 MiB (GPU 0; 23.65 GiB total capacity; 21.43 GiB already allocated; 11.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1638  :  64 one_to_one bias_attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1639  :  64 one_to_one bias_attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  1640  :  64 one_to_one bias_attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  1641  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1642  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1643  :  64 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1644  :  64 one_to_one attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 138.00 MiB (GPU 0; 23.65 GiB total capacity; 22.22 GiB already allocated; 81.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1645  :  64 one_to_one attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  1646  :  64 one_to_one attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  1647  :  64 one_to_one attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.56 GiB already allocated; 315.44 MiB free; 22.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1648  :  64 one_to_one attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.04 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1649  :  64 one_to_one attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.65 GiB total capacity; 21.87 GiB already allocated; 31.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1650  :  64 one_to_one bias_attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1651  :  64 one_to_one bias_attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  1652  :  64 one_to_one bias_attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  1653  :  64 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1654  :  64 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1655  :  64 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1656  :  64 one_to_one attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 23.65 GiB total capacity; 22.15 GiB already allocated; 93.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1657  :  64 one_to_one attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  1658  :  64 one_to_one attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  1659  :  64 one_to_one attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.92 GiB already allocated; 213.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1660  :  64 one_to_one attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.04 GiB already allocated; 47.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1661  :  64 one_to_one attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.65 GiB total capacity; 21.87 GiB already allocated; 49.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1662  :  64 one_to_one bias_attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1663  :  64 one_to_one bias_attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  1664  :  64 one_to_one bias_attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  1665  :  64 one_to_one bias_attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1666  :  64 one_to_one bias_attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1667  :  64 one_to_one bias_attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1668  :  64 one_to_one attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 23.65 GiB total capacity; 22.19 GiB already allocated; 87.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1669  :  64 one_to_one attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  1670  :  64 one_to_one attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  1671  :  64 one_to_one attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 7.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1672  :  64 one_to_one attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 23.65 GiB total capacity; 21.55 GiB already allocated; 11.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1673  :  64 one_to_one attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 556.00 MiB (GPU 0; 23.65 GiB total capacity; 21.43 GiB already allocated; 11.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1674  :  64 one_to_one bias_attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1675  :  64 one_to_one bias_attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  1676  :  64 one_to_one bias_attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  1677  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1678  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1679  :  64 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1680  :  64 one_to_one attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 23.65 GiB total capacity; 22.15 GiB already allocated; 93.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1681  :  64 one_to_one attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  1682  :  64 one_to_one attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  1683  :  64 one_to_one attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.92 GiB already allocated; 213.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1684  :  64 one_to_one attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.04 GiB already allocated; 47.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1685  :  64 one_to_one attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.65 GiB total capacity; 21.87 GiB already allocated; 49.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1686  :  64 one_to_one bias_attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1687  :  64 one_to_one bias_attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  1688  :  64 one_to_one bias_attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  1689  :  64 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1690  :  64 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1691  :  64 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1692  :  64 one_to_one attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 23.65 GiB total capacity; 22.15 GiB already allocated; 93.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1693  :  64 one_to_one attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  1694  :  64 one_to_one attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  1695  :  64 one_to_one attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.92 GiB already allocated; 213.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1696  :  64 one_to_one attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.04 GiB already allocated; 47.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1697  :  64 one_to_one attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.65 GiB total capacity; 21.87 GiB already allocated; 49.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1698  :  64 one_to_one bias_attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1699  :  64 one_to_one bias_attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  1700  :  64 one_to_one bias_attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  1701  :  64 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1702  :  64 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1703  :  64 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1704  :  64 one_to_one attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 23.65 GiB total capacity; 22.19 GiB already allocated; 87.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1705  :  64 one_to_one attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  1706  :  64 one_to_one attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  1707  :  64 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/home/janulm/janulm_agent_nodes/agent-net/util.py", line 50, in <forward op>

    out = matrix.index_select(-2, col)
    out = out * value.unsqueeze(-1)
          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    if reduce == 'sum':
        out = scatter_add(out, row, dim=-2, dim_size=m)
RuntimeError: CUDA out of memory. Tried to allocate 286.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 7.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


running  1708  :  64 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 432.00 MiB (GPU 0; 23.65 GiB total capacity; 21.55 GiB already allocated; 11.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1709  :  64 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 556.00 MiB (GPU 0; 23.65 GiB total capacity; 21.43 GiB already allocated; 11.44 MiB free; 22.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1710  :  64 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1711  :  64 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  1712  :  64 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  1713  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1714  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1715  :  64 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1716  :  64 one_to_one attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 134.00 MiB (GPU 0; 23.65 GiB total capacity; 22.15 GiB already allocated; 93.44 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1717  :  64 one_to_one attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  1718  :  64 one_to_one attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  1719  :  64 one_to_one attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.92 GiB already allocated; 213.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1720  :  64 one_to_one attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 166.00 MiB (GPU 0; 23.65 GiB total capacity; 22.04 GiB already allocated; 47.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1721  :  64 one_to_one attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 212.00 MiB (GPU 0; 23.65 GiB total capacity; 21.87 GiB already allocated; 49.44 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1722  :  64 one_to_one bias_attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1723  :  64 one_to_one bias_attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  1724  :  64 one_to_one bias_attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  1725  :  64 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1726  :  64 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1727  :  64 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=64, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 597.49 MiB already allocated; 21.85 GiB free; 680.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1728  :  128 one_to_one attention agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 541.44 MiB free; 21.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1729  :  128 one_to_one attention agent_start 0.01 3 0.0 2
skipped because reset not used
running  1730  :  128 one_to_one attention agent_start 0.01 3 0.0 3
skipped because reset not used
running  1731  :  128 one_to_one attention_reset agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 539.44 MiB free; 21.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1732  :  128 one_to_one attention_reset agent_start 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 541.44 MiB free; 21.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1733  :  128 one_to_one attention_reset agent_start 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 541.44 MiB free; 21.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1734  :  128 one_to_one bias_attention agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1735  :  128 one_to_one bias_attention agent_start 0.01 3 0.0 2
skipped because reset not used
running  1736  :  128 one_to_one bias_attention agent_start 0.01 3 0.0 3
skipped because reset not used
running  1737  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1738  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1739  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1740  :  128 one_to_one attention last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1741  :  128 one_to_one attention last_agent_visited 0.01 3 0.0 2
skipped because reset not used
running  1742  :  128 one_to_one attention last_agent_visited 0.01 3 0.0 3
skipped because reset not used
running  1743  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.56 GiB already allocated; 249.44 MiB free; 22.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1744  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.80 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1745  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.80 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1746  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1747  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.0 2
skipped because reset not used
running  1748  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.0 3
skipped because reset not used
running  1749  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1750  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1751  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1752  :  128 one_to_one attention node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1753  :  128 one_to_one attention node_embedding 0.01 3 0.0 2
skipped because reset not used
running  1754  :  128 one_to_one attention node_embedding 0.01 3 0.0 3
skipped because reset not used
running  1755  :  128 one_to_one attention_reset node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 581.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1756  :  128 one_to_one attention_reset node_embedding 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1757  :  128 one_to_one attention_reset node_embedding 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1758  :  128 one_to_one bias_attention node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1759  :  128 one_to_one bias_attention node_embedding 0.01 3 0.0 2
skipped because reset not used
running  1760  :  128 one_to_one bias_attention node_embedding 0.01 3 0.0 3
skipped because reset not used
running  1761  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1762  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1763  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1764  :  128 one_to_one attention agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1765  :  128 one_to_one attention agent_start 0.001 3 0.0 2
skipped because reset not used
running  1766  :  128 one_to_one attention agent_start 0.001 3 0.0 3
skipped because reset not used
running  1767  :  128 one_to_one attention_reset agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 581.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1768  :  128 one_to_one attention_reset agent_start 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1769  :  128 one_to_one attention_reset agent_start 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1770  :  128 one_to_one bias_attention agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1771  :  128 one_to_one bias_attention agent_start 0.001 3 0.0 2
skipped because reset not used
running  1772  :  128 one_to_one bias_attention agent_start 0.001 3 0.0 3
skipped because reset not used
running  1773  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1774  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1775  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1776  :  128 one_to_one attention last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1777  :  128 one_to_one attention last_agent_visited 0.001 3 0.0 2
skipped because reset not used
running  1778  :  128 one_to_one attention last_agent_visited 0.001 3 0.0 3
skipped because reset not used
running  1779  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.56 GiB already allocated; 249.44 MiB free; 22.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1780  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.80 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1781  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.80 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1782  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1783  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.0 2
skipped because reset not used
running  1784  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.0 3
skipped because reset not used
running  1785  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1786  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1787  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1788  :  128 one_to_one attention node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1789  :  128 one_to_one attention node_embedding 0.001 3 0.0 2
skipped because reset not used
running  1790  :  128 one_to_one attention node_embedding 0.001 3 0.0 3
skipped because reset not used
running  1791  :  128 one_to_one attention_reset node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 581.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1792  :  128 one_to_one attention_reset node_embedding 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1793  :  128 one_to_one attention_reset node_embedding 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1794  :  128 one_to_one bias_attention node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1795  :  128 one_to_one bias_attention node_embedding 0.001 3 0.0 2
skipped because reset not used
running  1796  :  128 one_to_one bias_attention node_embedding 0.001 3 0.0 3
skipped because reset not used
running  1797  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1798  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1799  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1800  :  128 one_to_one attention agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1801  :  128 one_to_one attention agent_start 0.0001 3 0.0 2
skipped because reset not used
running  1802  :  128 one_to_one attention agent_start 0.0001 3 0.0 3
skipped because reset not used
running  1803  :  128 one_to_one attention_reset agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 581.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1804  :  128 one_to_one attention_reset agent_start 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1805  :  128 one_to_one attention_reset agent_start 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1806  :  128 one_to_one bias_attention agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1807  :  128 one_to_one bias_attention agent_start 0.0001 3 0.0 2
skipped because reset not used
running  1808  :  128 one_to_one bias_attention agent_start 0.0001 3 0.0 3
skipped because reset not used
running  1809  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1810  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1811  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1812  :  128 one_to_one attention last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1813  :  128 one_to_one attention last_agent_visited 0.0001 3 0.0 2
skipped because reset not used
running  1814  :  128 one_to_one attention last_agent_visited 0.0001 3 0.0 3
skipped because reset not used
running  1815  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.56 GiB already allocated; 249.44 MiB free; 22.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1816  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.80 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1817  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.80 GiB already allocated; 251.44 MiB free; 22.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1818  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1819  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 2
skipped because reset not used
running  1820  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.0 3
skipped because reset not used
running  1821  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1822  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1823  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1824  :  128 one_to_one attention node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1825  :  128 one_to_one attention node_embedding 0.0001 3 0.0 2
skipped because reset not used
running  1826  :  128 one_to_one attention node_embedding 0.0001 3 0.0 3
skipped because reset not used
running  1827  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 581.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1828  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1829  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 856.00 MiB (GPU 0; 23.65 GiB total capacity; 20.67 GiB already allocated; 583.44 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1830  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1831  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.0 2
skipped because reset not used
running  1832  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.0 3
skipped because reset not used
running  1833  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1834  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1835  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1836  :  128 one_to_one attention agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.16 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1837  :  128 one_to_one attention agent_start 0.01 6 0.0 2
skipped because reset not used
running  1838  :  128 one_to_one attention agent_start 0.01 6 0.0 3
skipped because reset not used
running  1839  :  128 one_to_one attention_reset agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 575.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1840  :  128 one_to_one attention_reset agent_start 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1841  :  128 one_to_one attention_reset agent_start 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1842  :  128 one_to_one bias_attention agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1843  :  128 one_to_one bias_attention agent_start 0.01 6 0.0 2
skipped because reset not used
running  1844  :  128 one_to_one bias_attention agent_start 0.01 6 0.0 3
skipped because reset not used
running  1845  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1846  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1847  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1848  :  128 one_to_one attention last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 22.13 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1849  :  128 one_to_one attention last_agent_visited 0.01 6 0.0 2
skipped because reset not used
running  1850  :  128 one_to_one attention last_agent_visited 0.01 6 0.0 3
skipped because reset not used
running  1851  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.35 GiB already allocated; 77.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1852  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.25 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1853  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.25 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1854  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1855  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.0 2
skipped because reset not used
running  1856  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.0 3
skipped because reset not used
running  1857  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1858  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1859  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1860  :  128 one_to_one attention node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.16 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1861  :  128 one_to_one attention node_embedding 0.01 6 0.0 2
skipped because reset not used
running  1862  :  128 one_to_one attention node_embedding 0.01 6 0.0 3
skipped because reset not used
running  1863  :  128 one_to_one attention_reset node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 575.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1864  :  128 one_to_one attention_reset node_embedding 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1865  :  128 one_to_one attention_reset node_embedding 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1866  :  128 one_to_one bias_attention node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1867  :  128 one_to_one bias_attention node_embedding 0.01 6 0.0 2
skipped because reset not used
running  1868  :  128 one_to_one bias_attention node_embedding 0.01 6 0.0 3
skipped because reset not used
running  1869  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1870  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1871  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1872  :  128 one_to_one attention agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.16 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1873  :  128 one_to_one attention agent_start 0.001 6 0.0 2
skipped because reset not used
running  1874  :  128 one_to_one attention agent_start 0.001 6 0.0 3
skipped because reset not used
running  1875  :  128 one_to_one attention_reset agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 575.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1876  :  128 one_to_one attention_reset agent_start 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1877  :  128 one_to_one attention_reset agent_start 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1878  :  128 one_to_one bias_attention agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1879  :  128 one_to_one bias_attention agent_start 0.001 6 0.0 2
skipped because reset not used
running  1880  :  128 one_to_one bias_attention agent_start 0.001 6 0.0 3
skipped because reset not used
running  1881  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1882  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1883  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1884  :  128 one_to_one attention last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 22.13 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1885  :  128 one_to_one attention last_agent_visited 0.001 6 0.0 2
skipped because reset not used
running  1886  :  128 one_to_one attention last_agent_visited 0.001 6 0.0 3
skipped because reset not used
running  1887  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.35 GiB already allocated; 77.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1888  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.25 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1889  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.25 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1890  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1891  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.0 2
skipped because reset not used
running  1892  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.0 3
skipped because reset not used
running  1893  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1894  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1895  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1896  :  128 one_to_one attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.16 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1897  :  128 one_to_one attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  1898  :  128 one_to_one attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  1899  :  128 one_to_one attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 575.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1900  :  128 one_to_one attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1901  :  128 one_to_one attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1902  :  128 one_to_one bias_attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1903  :  128 one_to_one bias_attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  1904  :  128 one_to_one bias_attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  1905  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1906  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1907  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1908  :  128 one_to_one attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.16 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1909  :  128 one_to_one attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  1910  :  128 one_to_one attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  1911  :  128 one_to_one attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 575.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1912  :  128 one_to_one attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1913  :  128 one_to_one attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1914  :  128 one_to_one bias_attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1915  :  128 one_to_one bias_attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  1916  :  128 one_to_one bias_attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  1917  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1918  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1919  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1920  :  128 one_to_one attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 22.13 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1921  :  128 one_to_one attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  1922  :  128 one_to_one attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  1923  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.35 GiB already allocated; 77.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1924  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.25 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1925  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.25 GiB already allocated; 79.44 MiB free; 22.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1926  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1927  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  1928  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  1929  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1930  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1931  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1932  :  128 one_to_one attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 21.16 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1933  :  128 one_to_one attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  1934  :  128 one_to_one attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  1935  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 575.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1936  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1937  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 904.00 MiB (GPU 0; 23.65 GiB total capacity; 20.28 GiB already allocated; 577.44 MiB free; 21.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1938  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1939  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  1940  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  1941  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1942  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1943  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1944  :  128 one_to_one attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1945  :  128 one_to_one attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  1946  :  128 one_to_one attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  1947  :  128 one_to_one attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 173.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1948  :  128 one_to_one attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1949  :  128 one_to_one attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1950  :  128 one_to_one bias_attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1951  :  128 one_to_one bias_attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  1952  :  128 one_to_one bias_attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  1953  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1954  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1955  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1956  :  128 one_to_one attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.31 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1957  :  128 one_to_one attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  1958  :  128 one_to_one attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  1959  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 615.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1960  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 20.56 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1961  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 20.56 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1962  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1963  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  1964  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  1965  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1966  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1967  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1968  :  128 one_to_one attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1969  :  128 one_to_one attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  1970  :  128 one_to_one attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  1971  :  128 one_to_one attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 173.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1972  :  128 one_to_one attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1973  :  128 one_to_one attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1974  :  128 one_to_one bias_attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1975  :  128 one_to_one bias_attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  1976  :  128 one_to_one bias_attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  1977  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1978  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1979  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1980  :  128 one_to_one attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1981  :  128 one_to_one attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  1982  :  128 one_to_one attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  1983  :  128 one_to_one attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 173.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1984  :  128 one_to_one attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1985  :  128 one_to_one attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1986  :  128 one_to_one bias_attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1987  :  128 one_to_one bias_attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  1988  :  128 one_to_one bias_attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  1989  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1990  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1991  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1992  :  128 one_to_one attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.31 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1993  :  128 one_to_one attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  1994  :  128 one_to_one attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  1995  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 615.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1996  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 20.56 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1997  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 20.56 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1998  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1999  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  2000  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  2001  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2002  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2003  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2004  :  128 one_to_one attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2005  :  128 one_to_one attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  2006  :  128 one_to_one attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  2007  :  128 one_to_one attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 173.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2008  :  128 one_to_one attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2009  :  128 one_to_one attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2010  :  128 one_to_one bias_attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2011  :  128 one_to_one bias_attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  2012  :  128 one_to_one bias_attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  2013  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2014  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2015  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2016  :  128 one_to_one attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2017  :  128 one_to_one attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  2018  :  128 one_to_one attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  2019  :  128 one_to_one attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 173.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2020  :  128 one_to_one attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2021  :  128 one_to_one attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2022  :  128 one_to_one bias_attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2023  :  128 one_to_one bias_attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  2024  :  128 one_to_one bias_attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  2025  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2026  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2027  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2028  :  128 one_to_one attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.31 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2029  :  128 one_to_one attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  2030  :  128 one_to_one attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  2031  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.94 GiB already allocated; 615.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2032  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 20.56 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2033  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 20.56 GiB already allocated; 617.44 MiB free; 21.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2034  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2035  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  2036  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  2037  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2038  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2039  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2040  :  128 one_to_one attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2041  :  128 one_to_one attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  2042  :  128 one_to_one attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  2043  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 173.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2044  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2045  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 23.65 GiB total capacity; 21.10 GiB already allocated; 175.44 MiB free; 22.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2046  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2047  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  2048  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  2049  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2050  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2051  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2052  :  128 one_to_one attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2053  :  128 one_to_one attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  2054  :  128 one_to_one attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  2055  :  128 one_to_one attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.04 GiB already allocated; 209.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2056  :  128 one_to_one attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2057  :  128 one_to_one attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2058  :  128 one_to_one bias_attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2059  :  128 one_to_one bias_attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  2060  :  128 one_to_one bias_attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  2061  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2062  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2063  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2064  :  128 one_to_one attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 841.44 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2065  :  128 one_to_one attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  2066  :  128 one_to_one attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  2067  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 185.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2068  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 187.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2069  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 187.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2070  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2071  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  2072  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  2073  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2074  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2075  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2076  :  128 one_to_one attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 964.00 MiB (GPU 0; 23.65 GiB total capacity; 21.47 GiB already allocated; 205.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2077  :  128 one_to_one attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  2078  :  128 one_to_one attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  2079  :  128 one_to_one attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.04 GiB already allocated; 203.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2080  :  128 one_to_one attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 205.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2081  :  128 one_to_one attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 205.44 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2082  :  128 one_to_one bias_attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2083  :  128 one_to_one bias_attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  2084  :  128 one_to_one bias_attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  2085  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2086  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2087  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2088  :  128 one_to_one attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2089  :  128 one_to_one attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  2090  :  128 one_to_one attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  2091  :  128 one_to_one attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.04 GiB already allocated; 209.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2092  :  128 one_to_one attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2093  :  128 one_to_one attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2094  :  128 one_to_one bias_attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2095  :  128 one_to_one bias_attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  2096  :  128 one_to_one bias_attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  2097  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2098  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2099  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2100  :  128 one_to_one attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 841.44 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2101  :  128 one_to_one attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  2102  :  128 one_to_one attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  2103  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 185.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2104  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 187.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2105  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 187.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2106  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2107  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  2108  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  2109  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2110  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2111  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2112  :  128 one_to_one attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2113  :  128 one_to_one attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  2114  :  128 one_to_one attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  2115  :  128 one_to_one attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.04 GiB already allocated; 209.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2116  :  128 one_to_one attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2117  :  128 one_to_one attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2118  :  128 one_to_one bias_attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2119  :  128 one_to_one bias_attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  2120  :  128 one_to_one bias_attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  2121  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2122  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2123  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2124  :  128 one_to_one attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2125  :  128 one_to_one attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  2126  :  128 one_to_one attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  2127  :  128 one_to_one attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.04 GiB already allocated; 209.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2128  :  128 one_to_one attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2129  :  128 one_to_one attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2130  :  128 one_to_one bias_attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2131  :  128 one_to_one bias_attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  2132  :  128 one_to_one bias_attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  2133  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2134  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2135  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2136  :  128 one_to_one attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 964.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 837.44 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2137  :  128 one_to_one attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  2138  :  128 one_to_one attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  2139  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 181.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2140  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 964.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 183.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2141  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.49 GiB already allocated; 183.44 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2142  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2143  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  2144  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  2145  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2146  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2147  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2148  :  128 one_to_one attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 21.46 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2149  :  128 one_to_one attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  2150  :  128 one_to_one attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  2151  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.04 GiB already allocated; 209.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2152  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2153  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.65 GiB total capacity; 20.52 GiB already allocated; 211.44 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2154  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2155  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  2156  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  2157  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2158  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2159  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2160  :  128 one_to_one attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2161  :  128 one_to_one attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  2162  :  128 one_to_one attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  2163  :  128 one_to_one attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 145.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2164  :  128 one_to_one attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2165  :  128 one_to_one attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2166  :  128 one_to_one bias_attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2167  :  128 one_to_one bias_attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  2168  :  128 one_to_one bias_attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  2169  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2170  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2171  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2172  :  128 one_to_one attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.75 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2173  :  128 one_to_one attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  2174  :  128 one_to_one attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  2175  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.29 GiB already allocated; 445.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2176  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.89 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2177  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.89 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2178  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2179  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  2180  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  2181  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2182  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2183  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2184  :  128 one_to_one attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2185  :  128 one_to_one attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  2186  :  128 one_to_one attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  2187  :  128 one_to_one attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 145.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2188  :  128 one_to_one attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2189  :  128 one_to_one attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2190  :  128 one_to_one bias_attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2191  :  128 one_to_one bias_attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  2192  :  128 one_to_one bias_attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  2193  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2194  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2195  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2196  :  128 one_to_one attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2197  :  128 one_to_one attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  2198  :  128 one_to_one attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  2199  :  128 one_to_one attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 145.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2200  :  128 one_to_one attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2201  :  128 one_to_one attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2202  :  128 one_to_one bias_attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2203  :  128 one_to_one bias_attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  2204  :  128 one_to_one bias_attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  2205  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2206  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2207  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2208  :  128 one_to_one attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.75 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2209  :  128 one_to_one attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  2210  :  128 one_to_one attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  2211  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.29 GiB already allocated; 445.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2212  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.89 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2213  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.89 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2214  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2215  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  2216  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  2217  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2218  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2219  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2220  :  128 one_to_one attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2221  :  128 one_to_one attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  2222  :  128 one_to_one attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  2223  :  128 one_to_one attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 145.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2224  :  128 one_to_one attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2225  :  128 one_to_one attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2226  :  128 one_to_one bias_attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2227  :  128 one_to_one bias_attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  2228  :  128 one_to_one bias_attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  2229  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2230  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2231  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2232  :  128 one_to_one attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2233  :  128 one_to_one attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  2234  :  128 one_to_one attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  2235  :  128 one_to_one attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 145.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2236  :  128 one_to_one attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2237  :  128 one_to_one attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2238  :  128 one_to_one bias_attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2239  :  128 one_to_one bias_attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  2240  :  128 one_to_one bias_attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  2241  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2242  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2243  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2244  :  128 one_to_one attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.75 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2245  :  128 one_to_one attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  2246  :  128 one_to_one attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  2247  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.29 GiB already allocated; 445.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2248  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.89 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2249  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.89 GiB already allocated; 447.44 MiB free; 22.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2250  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2251  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  2252  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  2253  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2254  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2255  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2256  :  128 one_to_one attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2257  :  128 one_to_one attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  2258  :  128 one_to_one attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  2259  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.65 GiB total capacity; 21.60 GiB already allocated; 145.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2260  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2261  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 882.00 MiB (GPU 0; 23.65 GiB total capacity; 20.79 GiB already allocated; 147.44 MiB free; 22.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2262  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2263  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  2264  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  2265  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2266  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2267  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2268  :  128 one_to_one attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 683.44 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2269  :  128 one_to_one attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  2270  :  128 one_to_one attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  2271  :  128 one_to_one attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 27.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2272  :  128 one_to_one attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2273  :  128 one_to_one attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2274  :  128 one_to_one bias_attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2275  :  128 one_to_one bias_attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  2276  :  128 one_to_one bias_attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  2277  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2278  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2279  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2280  :  128 one_to_one attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 22.03 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2281  :  128 one_to_one attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  2282  :  128 one_to_one attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  2283  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.48 GiB already allocated; 265.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2284  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.11 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2285  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.11 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2286  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2287  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  2288  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  2289  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2290  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2291  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2292  :  128 one_to_one attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 683.44 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2293  :  128 one_to_one attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  2294  :  128 one_to_one attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  2295  :  128 one_to_one attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 27.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2296  :  128 one_to_one attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2297  :  128 one_to_one attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2298  :  128 one_to_one bias_attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2299  :  128 one_to_one bias_attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  2300  :  128 one_to_one bias_attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  2301  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2302  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2303  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2304  :  128 one_to_one attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 683.44 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2305  :  128 one_to_one attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  2306  :  128 one_to_one attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  2307  :  128 one_to_one attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 27.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2308  :  128 one_to_one attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2309  :  128 one_to_one attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2310  :  128 one_to_one bias_attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2311  :  128 one_to_one bias_attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  2312  :  128 one_to_one bias_attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  2313  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2314  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2315  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2316  :  128 one_to_one attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 22.03 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2317  :  128 one_to_one attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  2318  :  128 one_to_one attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  2319  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.48 GiB already allocated; 265.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2320  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.11 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2321  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.11 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2322  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2323  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  2324  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  2325  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2326  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2327  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2328  :  128 one_to_one attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 683.44 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2329  :  128 one_to_one attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  2330  :  128 one_to_one attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  2331  :  128 one_to_one attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 27.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2332  :  128 one_to_one attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2333  :  128 one_to_one attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2334  :  128 one_to_one bias_attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2335  :  128 one_to_one bias_attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  2336  :  128 one_to_one bias_attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  2337  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2338  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2339  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2340  :  128 one_to_one attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 683.44 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2341  :  128 one_to_one attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  2342  :  128 one_to_one attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  2343  :  128 one_to_one attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 27.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2344  :  128 one_to_one attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2345  :  128 one_to_one attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2346  :  128 one_to_one bias_attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2347  :  128 one_to_one bias_attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  2348  :  128 one_to_one bias_attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  2349  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2350  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2351  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2352  :  128 one_to_one attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 22.03 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2353  :  128 one_to_one attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  2354  :  128 one_to_one attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  2355  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.48 GiB already allocated; 265.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2356  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.11 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2357  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.11 GiB already allocated; 267.44 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2358  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2359  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  2360  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  2361  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2362  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2363  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2364  :  128 one_to_one attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 683.44 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2365  :  128 one_to_one attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  2366  :  128 one_to_one attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  2367  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.23 GiB already allocated; 27.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2368  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2369  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.65 GiB total capacity; 21.06 GiB already allocated; 29.44 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2370  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2371  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  2372  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  2373  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2374  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2375  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2376  :  128 one_to_one attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2377  :  128 one_to_one attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  2378  :  128 one_to_one attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  2379  :  128 one_to_one attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.02 GiB already allocated; 295.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2380  :  128 one_to_one attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2381  :  128 one_to_one attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2382  :  128 one_to_one bias_attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2383  :  128 one_to_one bias_attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  2384  :  128 one_to_one bias_attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  2385  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2386  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2387  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2388  :  128 one_to_one attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.67 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2389  :  128 one_to_one attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  2390  :  128 one_to_one attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  2391  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.28 GiB already allocated; 561.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2392  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.84 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2393  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.84 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2394  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2395  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  2396  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  2397  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2398  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2399  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2400  :  128 one_to_one attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2401  :  128 one_to_one attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  2402  :  128 one_to_one attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  2403  :  128 one_to_one attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.02 GiB already allocated; 295.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2404  :  128 one_to_one attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2405  :  128 one_to_one attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2406  :  128 one_to_one bias_attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2407  :  128 one_to_one bias_attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  2408  :  128 one_to_one bias_attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  2409  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2410  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2411  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2412  :  128 one_to_one attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2413  :  128 one_to_one attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  2414  :  128 one_to_one attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  2415  :  128 one_to_one attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.02 GiB already allocated; 295.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2416  :  128 one_to_one attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2417  :  128 one_to_one attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2418  :  128 one_to_one bias_attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2419  :  128 one_to_one bias_attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  2420  :  128 one_to_one bias_attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  2421  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2422  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2423  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2424  :  128 one_to_one attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.67 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2425  :  128 one_to_one attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  2426  :  128 one_to_one attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  2427  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.28 GiB already allocated; 561.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2428  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.84 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2429  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.84 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2430  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2431  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  2432  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  2433  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2434  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2435  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2436  :  128 one_to_one attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2437  :  128 one_to_one attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  2438  :  128 one_to_one attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  2439  :  128 one_to_one attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.02 GiB already allocated; 295.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2440  :  128 one_to_one attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2441  :  128 one_to_one attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2442  :  128 one_to_one bias_attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2443  :  128 one_to_one bias_attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  2444  :  128 one_to_one bias_attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  2445  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2446  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2447  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2448  :  128 one_to_one attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2449  :  128 one_to_one attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  2450  :  128 one_to_one attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  2451  :  128 one_to_one attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.02 GiB already allocated; 295.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2452  :  128 one_to_one attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2453  :  128 one_to_one attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2454  :  128 one_to_one bias_attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2455  :  128 one_to_one bias_attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  2456  :  128 one_to_one bias_attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  2457  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2458  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2459  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2460  :  128 one_to_one attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.67 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2461  :  128 one_to_one attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  2462  :  128 one_to_one attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  2463  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.28 GiB already allocated; 561.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2464  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.84 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2465  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.84 GiB already allocated; 563.44 MiB free; 21.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2466  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2467  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  2468  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  2469  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2470  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2471  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2472  :  128 one_to_one attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 21.53 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2473  :  128 one_to_one attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  2474  :  128 one_to_one attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  2475  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.02 GiB already allocated; 295.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2476  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2477  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.65 GiB total capacity; 20.70 GiB already allocated; 297.44 MiB free; 22.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2478  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2479  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  2480  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  2481  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2482  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2483  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2484  :  128 one_to_one attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.21 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2485  :  128 one_to_one attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  2486  :  128 one_to_one attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  2487  :  128 one_to_one attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 523.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2488  :  128 one_to_one attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2489  :  128 one_to_one attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2490  :  128 one_to_one bias_attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2491  :  128 one_to_one bias_attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  2492  :  128 one_to_one bias_attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  2493  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2494  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2495  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2496  :  128 one_to_one attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 895.44 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2497  :  128 one_to_one attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  2498  :  128 one_to_one attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  2499  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.55 GiB already allocated; 229.44 MiB free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2500  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 231.44 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2501  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 231.44 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2502  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2503  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  2504  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  2505  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2506  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2507  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2508  :  128 one_to_one attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.21 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2509  :  128 one_to_one attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  2510  :  128 one_to_one attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  2511  :  128 one_to_one attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 523.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2512  :  128 one_to_one attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2513  :  128 one_to_one attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2514  :  128 one_to_one bias_attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2515  :  128 one_to_one bias_attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  2516  :  128 one_to_one bias_attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  2517  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2518  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2519  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2520  :  128 one_to_one attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.21 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2521  :  128 one_to_one attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  2522  :  128 one_to_one attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  2523  :  128 one_to_one attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 523.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2524  :  128 one_to_one attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2525  :  128 one_to_one attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2526  :  128 one_to_one bias_attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2527  :  128 one_to_one bias_attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  2528  :  128 one_to_one bias_attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  2529  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2530  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2531  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2532  :  128 one_to_one attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 895.44 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2533  :  128 one_to_one attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  2534  :  128 one_to_one attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  2535  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.55 GiB already allocated; 229.44 MiB free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2536  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 231.44 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2537  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 231.44 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2538  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2539  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  2540  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  2541  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2542  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2543  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2544  :  128 one_to_one attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.21 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2545  :  128 one_to_one attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  2546  :  128 one_to_one attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  2547  :  128 one_to_one attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 523.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2548  :  128 one_to_one attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2549  :  128 one_to_one attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2550  :  128 one_to_one bias_attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2551  :  128 one_to_one bias_attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  2552  :  128 one_to_one bias_attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  2553  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2554  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2555  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2556  :  128 one_to_one attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.21 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2557  :  128 one_to_one attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  2558  :  128 one_to_one attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  2559  :  128 one_to_one attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 523.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2560  :  128 one_to_one attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2561  :  128 one_to_one attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2562  :  128 one_to_one bias_attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2563  :  128 one_to_one bias_attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  2564  :  128 one_to_one bias_attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  2565  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2566  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2567  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2568  :  128 one_to_one attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 895.44 MiB free; 21.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2569  :  128 one_to_one attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  2570  :  128 one_to_one attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  2571  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 21.55 GiB already allocated; 229.44 MiB free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2572  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 231.44 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2573  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.24 GiB already allocated; 231.44 MiB free; 22.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2574  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2575  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  2576  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  2577  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2578  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2579  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2580  :  128 one_to_one attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 21.21 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2581  :  128 one_to_one attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  2582  :  128 one_to_one attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  2583  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.65 GiB total capacity; 20.59 GiB already allocated; 523.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2584  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2585  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 23.65 GiB total capacity; 20.27 GiB already allocated; 525.44 MiB free; 22.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2586  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2587  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  2588  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  2589  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2590  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2591  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.65 GiB total capacity; 889.43 MiB already allocated; 21.56 GiB free; 974.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
