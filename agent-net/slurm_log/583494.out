Running on host: tikgpu07
In directory: /home/janulm/janulm_agent_nodes/agent-net
Starting on: Mon Jan 2 17:02:30 CET 2023
SLURM_JOB_ID: 583494
running  1900  :  128 one_to_one attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 900.00 MiB (GPU 0; 23.70 GiB total capacity; 21.05 GiB already allocated; 91.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1901  :  128 one_to_one attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 91.69 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1902  :  128 one_to_one bias_attention node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 891.16 MiB already allocated; 21.01 GiB free; 898.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1903  :  128 one_to_one bias_attention node_embedding 0.001 6 0.0 2
skipped because reset not used
running  1904  :  128 one_to_one bias_attention node_embedding 0.001 6 0.0 3
skipped because reset not used
running  1905  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1906  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1907  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1908  :  128 one_to_one attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 21.13 GiB already allocated; 27.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1909  :  128 one_to_one attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  1910  :  128 one_to_one attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  1911  :  128 one_to_one attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 25.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1912  :  128 one_to_one attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 27.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1913  :  128 one_to_one attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 27.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1914  :  128 one_to_one bias_attention agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1915  :  128 one_to_one bias_attention agent_start 0.0001 6 0.0 2
skipped because reset not used
running  1916  :  128 one_to_one bias_attention agent_start 0.0001 6 0.0 3
skipped because reset not used
running  1917  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1918  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1919  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1920  :  128 one_to_one attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 21.22 GiB already allocated; 427.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1921  :  128 one_to_one attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  1922  :  128 one_to_one attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  1923  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.62 GiB already allocated; 425.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1924  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.34 GiB already allocated; 427.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1925  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.34 GiB already allocated; 427.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1926  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1927  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 2
skipped because reset not used
running  1928  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.0 3
skipped because reset not used
running  1929  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1930  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1931  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1932  :  128 one_to_one attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 21.13 GiB already allocated; 27.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1933  :  128 one_to_one attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  1934  :  128 one_to_one attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  1935  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 25.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1936  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 27.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1937  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 898.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 27.69 MiB free; 21.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1938  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1939  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.0 2
skipped because reset not used
running  1940  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.0 3
skipped because reset not used
running  1941  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1942  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1943  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1944  :  128 one_to_one attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1945  :  128 one_to_one attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  1946  :  128 one_to_one attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  1947  :  128 one_to_one attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 429.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1948  :  128 one_to_one attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1949  :  128 one_to_one attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1950  :  128 one_to_one bias_attention agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1951  :  128 one_to_one bias_attention agent_start 0.01 9 0.0 2
skipped because reset not used
running  1952  :  128 one_to_one bias_attention agent_start 0.01 9 0.0 3
skipped because reset not used
running  1953  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1954  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1955  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1956  :  128 one_to_one attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 21.24 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1957  :  128 one_to_one attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  1958  :  128 one_to_one attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  1959  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.90 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1960  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.50 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1961  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.50 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1962  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1963  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.0 2
skipped because reset not used
running  1964  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.0 3
skipped because reset not used
running  1965  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1966  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1967  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1968  :  128 one_to_one attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1969  :  128 one_to_one attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  1970  :  128 one_to_one attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  1971  :  128 one_to_one attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 429.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1972  :  128 one_to_one attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1973  :  128 one_to_one attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1974  :  128 one_to_one bias_attention node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1975  :  128 one_to_one bias_attention node_embedding 0.01 9 0.0 2
skipped because reset not used
running  1976  :  128 one_to_one bias_attention node_embedding 0.01 9 0.0 3
skipped because reset not used
running  1977  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1978  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1979  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1980  :  128 one_to_one attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1981  :  128 one_to_one attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  1982  :  128 one_to_one attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  1983  :  128 one_to_one attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 429.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1984  :  128 one_to_one attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1985  :  128 one_to_one attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1986  :  128 one_to_one bias_attention agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1987  :  128 one_to_one bias_attention agent_start 0.001 9 0.0 2
skipped because reset not used
running  1988  :  128 one_to_one bias_attention agent_start 0.001 9 0.0 3
skipped because reset not used
running  1989  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1990  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1991  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1992  :  128 one_to_one attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 21.24 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1993  :  128 one_to_one attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  1994  :  128 one_to_one attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  1995  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.90 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1996  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.50 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1997  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.50 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1998  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  1999  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.0 2
skipped because reset not used
running  2000  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.0 3
skipped because reset not used
running  2001  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2002  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2003  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2004  :  128 one_to_one attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2005  :  128 one_to_one attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  2006  :  128 one_to_one attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  2007  :  128 one_to_one attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 429.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2008  :  128 one_to_one attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2009  :  128 one_to_one attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2010  :  128 one_to_one bias_attention node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2011  :  128 one_to_one bias_attention node_embedding 0.001 9 0.0 2
skipped because reset not used
running  2012  :  128 one_to_one bias_attention node_embedding 0.001 9 0.0 3
skipped because reset not used
running  2013  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2014  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2015  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2016  :  128 one_to_one attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2017  :  128 one_to_one attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  2018  :  128 one_to_one attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  2019  :  128 one_to_one attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 429.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2020  :  128 one_to_one attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2021  :  128 one_to_one attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2022  :  128 one_to_one bias_attention agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2023  :  128 one_to_one bias_attention agent_start 0.0001 9 0.0 2
skipped because reset not used
running  2024  :  128 one_to_one bias_attention agent_start 0.0001 9 0.0 3
skipped because reset not used
running  2025  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2026  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2027  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2028  :  128 one_to_one attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 21.24 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2029  :  128 one_to_one attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  2030  :  128 one_to_one attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  2031  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.90 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2032  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.50 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2033  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.50 GiB already allocated; 99.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2034  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2035  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 2
skipped because reset not used
running  2036  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.0 3
skipped because reset not used
running  2037  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2038  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2039  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2040  :  128 one_to_one attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2041  :  128 one_to_one attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  2042  :  128 one_to_one attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  2043  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 429.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2044  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2045  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 762.00 MiB (GPU 0; 23.70 GiB total capacity; 20.27 GiB already allocated; 431.69 MiB free; 21.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2046  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2047  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.0 2
skipped because reset not used
running  2048  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.0 3
skipped because reset not used
running  2049  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2050  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2051  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2052  :  128 one_to_one attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.55 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2053  :  128 one_to_one attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  2054  :  128 one_to_one attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  2055  :  128 one_to_one attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.52 GiB already allocated; 531.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2056  :  128 one_to_one attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2057  :  128 one_to_one attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2058  :  128 one_to_one bias_attention agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2059  :  128 one_to_one bias_attention agent_start 0.01 18 0.0 2
skipped because reset not used
running  2060  :  128 one_to_one bias_attention agent_start 0.01 18 0.0 3
skipped because reset not used
running  2061  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2062  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2063  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2064  :  128 one_to_one attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 21.52 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2065  :  128 one_to_one attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  2066  :  128 one_to_one attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  2067  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.77 GiB already allocated; 199.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2068  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.57 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2069  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.57 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2070  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2071  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.0 2
skipped because reset not used
running  2072  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.0 3
skipped because reset not used
running  2073  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2074  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2075  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2076  :  128 one_to_one attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.55 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2077  :  128 one_to_one attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  2078  :  128 one_to_one attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  2079  :  128 one_to_one attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.52 GiB already allocated; 531.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2080  :  128 one_to_one attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2081  :  128 one_to_one attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2082  :  128 one_to_one bias_attention node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2083  :  128 one_to_one bias_attention node_embedding 0.01 18 0.0 2
skipped because reset not used
running  2084  :  128 one_to_one bias_attention node_embedding 0.01 18 0.0 3
skipped because reset not used
running  2085  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2086  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2087  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2088  :  128 one_to_one attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.55 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2089  :  128 one_to_one attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  2090  :  128 one_to_one attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  2091  :  128 one_to_one attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.52 GiB already allocated; 531.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2092  :  128 one_to_one attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2093  :  128 one_to_one attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2094  :  128 one_to_one bias_attention agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2095  :  128 one_to_one bias_attention agent_start 0.001 18 0.0 2
skipped because reset not used
running  2096  :  128 one_to_one bias_attention agent_start 0.001 18 0.0 3
skipped because reset not used
running  2097  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2098  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2099  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2100  :  128 one_to_one attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 21.52 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2101  :  128 one_to_one attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  2102  :  128 one_to_one attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  2103  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.77 GiB already allocated; 199.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2104  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.57 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2105  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.57 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2106  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2107  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.0 2
skipped because reset not used
running  2108  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.0 3
skipped because reset not used
running  2109  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2110  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2111  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2112  :  128 one_to_one attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.55 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2113  :  128 one_to_one attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  2114  :  128 one_to_one attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  2115  :  128 one_to_one attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.52 GiB already allocated; 531.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2116  :  128 one_to_one attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2117  :  128 one_to_one attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2118  :  128 one_to_one bias_attention node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2119  :  128 one_to_one bias_attention node_embedding 0.001 18 0.0 2
skipped because reset not used
running  2120  :  128 one_to_one bias_attention node_embedding 0.001 18 0.0 3
skipped because reset not used
running  2121  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2122  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2123  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2124  :  128 one_to_one attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.55 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2125  :  128 one_to_one attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  2126  :  128 one_to_one attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  2127  :  128 one_to_one attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.52 GiB already allocated; 531.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2128  :  128 one_to_one attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2129  :  128 one_to_one attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2130  :  128 one_to_one bias_attention agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2131  :  128 one_to_one bias_attention agent_start 0.0001 18 0.0 2
skipped because reset not used
running  2132  :  128 one_to_one bias_attention agent_start 0.0001 18 0.0 3
skipped because reset not used
running  2133  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2134  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2135  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2136  :  128 one_to_one attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 21.52 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2137  :  128 one_to_one attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  2138  :  128 one_to_one attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  2139  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.77 GiB already allocated; 199.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2140  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.57 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2141  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.57 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2142  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2143  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 2
skipped because reset not used
running  2144  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.0 3
skipped because reset not used
running  2145  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2146  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2147  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2148  :  128 one_to_one attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 20.55 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2149  :  128 one_to_one attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  2150  :  128 one_to_one attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  2151  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.52 GiB already allocated; 531.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2152  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2153  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 23.70 GiB total capacity; 19.60 GiB already allocated; 533.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2154  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2155  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.0 2
skipped because reset not used
running  2156  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.0 3
skipped because reset not used
running  2157  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2158  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2159  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.0 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.0, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2160  :  128 one_to_one attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2161  :  128 one_to_one attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  2162  :  128 one_to_one attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  2163  :  128 one_to_one attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.35 GiB already allocated; 539.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2164  :  128 one_to_one attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2165  :  128 one_to_one attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2166  :  128 one_to_one bias_attention agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2167  :  128 one_to_one bias_attention agent_start 0.01 3 0.3 2
skipped because reset not used
running  2168  :  128 one_to_one bias_attention agent_start 0.01 3 0.3 3
skipped because reset not used
running  2169  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2170  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2171  :  128 one_to_one bias_attention_reset agent_start 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2172  :  128 one_to_one attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 761.69 MiB free; 21.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2173  :  128 one_to_one attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  2174  :  128 one_to_one attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  2175  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.33 GiB already allocated; 95.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2176  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2177  :  128 one_to_one attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2178  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2179  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.3 2
skipped because reset not used
running  2180  :  128 one_to_one bias_attention last_agent_visited 0.01 3 0.3 3
skipped because reset not used
running  2181  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2182  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2183  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2184  :  128 one_to_one attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2185  :  128 one_to_one attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  2186  :  128 one_to_one attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  2187  :  128 one_to_one attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.35 GiB already allocated; 539.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2188  :  128 one_to_one attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2189  :  128 one_to_one attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2190  :  128 one_to_one bias_attention node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2191  :  128 one_to_one bias_attention node_embedding 0.01 3 0.3 2
skipped because reset not used
running  2192  :  128 one_to_one bias_attention node_embedding 0.01 3 0.3 3
skipped because reset not used
running  2193  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2194  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2195  :  128 one_to_one bias_attention_reset node_embedding 0.01 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2196  :  128 one_to_one attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2197  :  128 one_to_one attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  2198  :  128 one_to_one attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  2199  :  128 one_to_one attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.35 GiB already allocated; 539.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2200  :  128 one_to_one attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2201  :  128 one_to_one attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2202  :  128 one_to_one bias_attention agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2203  :  128 one_to_one bias_attention agent_start 0.001 3 0.3 2
skipped because reset not used
running  2204  :  128 one_to_one bias_attention agent_start 0.001 3 0.3 3
skipped because reset not used
running  2205  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2206  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2207  :  128 one_to_one bias_attention_reset agent_start 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2208  :  128 one_to_one attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 761.69 MiB free; 21.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2209  :  128 one_to_one attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  2210  :  128 one_to_one attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  2211  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.33 GiB already allocated; 95.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2212  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2213  :  128 one_to_one attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2214  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2215  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.3 2
skipped because reset not used
running  2216  :  128 one_to_one bias_attention last_agent_visited 0.001 3 0.3 3
skipped because reset not used
running  2217  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2218  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2219  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2220  :  128 one_to_one attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2221  :  128 one_to_one attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  2222  :  128 one_to_one attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  2223  :  128 one_to_one attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.35 GiB already allocated; 539.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2224  :  128 one_to_one attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2225  :  128 one_to_one attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2226  :  128 one_to_one bias_attention node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2227  :  128 one_to_one bias_attention node_embedding 0.001 3 0.3 2
skipped because reset not used
running  2228  :  128 one_to_one bias_attention node_embedding 0.001 3 0.3 3
skipped because reset not used
running  2229  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2230  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2231  :  128 one_to_one bias_attention_reset node_embedding 0.001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2232  :  128 one_to_one attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2233  :  128 one_to_one attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  2234  :  128 one_to_one attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  2235  :  128 one_to_one attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.35 GiB already allocated; 539.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2236  :  128 one_to_one attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2237  :  128 one_to_one attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2238  :  128 one_to_one bias_attention agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2239  :  128 one_to_one bias_attention agent_start 0.0001 3 0.3 2
skipped because reset not used
running  2240  :  128 one_to_one bias_attention agent_start 0.0001 3 0.3 3
skipped because reset not used
running  2241  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2242  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2243  :  128 one_to_one bias_attention_reset agent_start 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2244  :  128 one_to_one attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 761.69 MiB free; 21.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2245  :  128 one_to_one attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  2246  :  128 one_to_one attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  2247  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.33 GiB already allocated; 95.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2248  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2249  :  128 one_to_one attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.92 GiB already allocated; 97.69 MiB free; 21.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2250  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2251  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 2
skipped because reset not used
running  2252  :  128 one_to_one bias_attention last_agent_visited 0.0001 3 0.3 3
skipped because reset not used
running  2253  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2254  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2255  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2256  :  128 one_to_one attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 20.82 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2257  :  128 one_to_one attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  2258  :  128 one_to_one attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  2259  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.35 GiB already allocated; 539.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2260  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2261  :  128 one_to_one attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 884.00 MiB (GPU 0; 23.70 GiB total capacity; 19.95 GiB already allocated; 541.69 MiB free; 21.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2262  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2263  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.3 2
skipped because reset not used
running  2264  :  128 one_to_one bias_attention node_embedding 0.0001 3 0.3 3
skipped because reset not used
running  2265  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2266  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2267  :  128 one_to_one bias_attention_reset node_embedding 0.0001 3 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=3, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2268  :  128 one_to_one attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.04 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2269  :  128 one_to_one attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  2270  :  128 one_to_one attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  2271  :  128 one_to_one attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 269.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2272  :  128 one_to_one attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2273  :  128 one_to_one attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2274  :  128 one_to_one bias_attention agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2275  :  128 one_to_one bias_attention agent_start 0.01 6 0.3 2
skipped because reset not used
running  2276  :  128 one_to_one bias_attention agent_start 0.01 6 0.3 3
skipped because reset not used
running  2277  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2278  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2279  :  128 one_to_one bias_attention_reset agent_start 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2280  :  128 one_to_one attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 535.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2281  :  128 one_to_one attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  2282  :  128 one_to_one attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  2283  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 23.70 GiB total capacity; 20.93 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2284  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 203.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2285  :  128 one_to_one attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 203.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2286  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2287  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.3 2
skipped because reset not used
running  2288  :  128 one_to_one bias_attention last_agent_visited 0.01 6 0.3 3
skipped because reset not used
running  2289  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2290  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2291  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2292  :  128 one_to_one attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2293  :  128 one_to_one attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  2294  :  128 one_to_one attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  2295  :  128 one_to_one attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 269.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2296  :  128 one_to_one attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2297  :  128 one_to_one attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2298  :  128 one_to_one bias_attention node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2299  :  128 one_to_one bias_attention node_embedding 0.01 6 0.3 2
skipped because reset not used
running  2300  :  128 one_to_one bias_attention node_embedding 0.01 6 0.3 3
skipped because reset not used
running  2301  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2302  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2303  :  128 one_to_one bias_attention_reset node_embedding 0.01 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2304  :  128 one_to_one attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2305  :  128 one_to_one attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  2306  :  128 one_to_one attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  2307  :  128 one_to_one attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 269.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2308  :  128 one_to_one attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2309  :  128 one_to_one attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2310  :  128 one_to_one bias_attention agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2311  :  128 one_to_one bias_attention agent_start 0.001 6 0.3 2
skipped because reset not used
running  2312  :  128 one_to_one bias_attention agent_start 0.001 6 0.3 3
skipped because reset not used
running  2313  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2314  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2315  :  128 one_to_one bias_attention_reset agent_start 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2316  :  128 one_to_one attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 535.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2317  :  128 one_to_one attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  2318  :  128 one_to_one attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  2319  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 23.70 GiB total capacity; 20.93 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2320  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 203.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2321  :  128 one_to_one attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 203.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2322  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2323  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.3 2
skipped because reset not used
running  2324  :  128 one_to_one bias_attention last_agent_visited 0.001 6 0.3 3
skipped because reset not used
running  2325  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2326  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2327  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2328  :  128 one_to_one attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2329  :  128 one_to_one attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  2330  :  128 one_to_one attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  2331  :  128 one_to_one attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 269.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2332  :  128 one_to_one attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2333  :  128 one_to_one attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2334  :  128 one_to_one bias_attention node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2335  :  128 one_to_one bias_attention node_embedding 0.001 6 0.3 2
skipped because reset not used
running  2336  :  128 one_to_one bias_attention node_embedding 0.001 6 0.3 3
skipped because reset not used
running  2337  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2338  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2339  :  128 one_to_one bias_attention_reset node_embedding 0.001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2340  :  128 one_to_one attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.04 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2341  :  128 one_to_one attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  2342  :  128 one_to_one attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  2343  :  128 one_to_one attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 269.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2344  :  128 one_to_one attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2345  :  128 one_to_one attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 271.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2346  :  128 one_to_one bias_attention agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2347  :  128 one_to_one bias_attention agent_start 0.0001 6 0.3 2
skipped because reset not used
running  2348  :  128 one_to_one bias_attention agent_start 0.0001 6 0.3 3
skipped because reset not used
running  2349  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2350  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2351  :  128 one_to_one bias_attention_reset agent_start 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2352  :  128 one_to_one attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 535.69 MiB free; 21.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2353  :  128 one_to_one attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  2354  :  128 one_to_one attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  2355  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 23.70 GiB total capacity; 20.93 GiB already allocated; 201.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2356  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 203.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2357  :  128 one_to_one attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 23.70 GiB total capacity; 21.09 GiB already allocated; 203.69 MiB free; 21.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2358  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2359  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 2
skipped because reset not used
running  2360  :  128 one_to_one bias_attention last_agent_visited 0.0001 6 0.3 3
skipped because reset not used
running  2361  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2362  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2363  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2364  :  128 one_to_one attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 926.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 275.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2365  :  128 one_to_one attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  2366  :  128 one_to_one attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  2367  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.49 GiB already allocated; 273.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2368  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 275.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2369  :  128 one_to_one attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 928.00 MiB (GPU 0; 23.70 GiB total capacity; 20.12 GiB already allocated; 275.69 MiB free; 21.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2370  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2371  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.3 2
skipped because reset not used
running  2372  :  128 one_to_one bias_attention node_embedding 0.0001 6 0.3 3
skipped because reset not used
running  2373  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2374  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2375  :  128 one_to_one bias_attention_reset node_embedding 0.0001 6 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=6, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2376  :  128 one_to_one attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.69 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2377  :  128 one_to_one attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  2378  :  128 one_to_one attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  2379  :  128 one_to_one attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.31 GiB already allocated; 381.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2380  :  128 one_to_one attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2381  :  128 one_to_one attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2382  :  128 one_to_one bias_attention agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2383  :  128 one_to_one bias_attention agent_start 0.01 9 0.3 2
skipped because reset not used
running  2384  :  128 one_to_one bias_attention agent_start 0.01 9 0.3 3
skipped because reset not used
running  2385  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2386  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2387  :  128 one_to_one bias_attention_reset agent_start 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2388  :  128 one_to_one attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 721.69 MiB free; 21.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2389  :  128 one_to_one attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  2390  :  128 one_to_one attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  2391  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.28 GiB already allocated; 55.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2392  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2393  :  128 one_to_one attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2394  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2395  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.3 2
skipped because reset not used
running  2396  :  128 one_to_one bias_attention last_agent_visited 0.01 9 0.3 3
skipped because reset not used
running  2397  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2398  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2399  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2400  :  128 one_to_one attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.69 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2401  :  128 one_to_one attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  2402  :  128 one_to_one attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  2403  :  128 one_to_one attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.31 GiB already allocated; 381.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2404  :  128 one_to_one attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2405  :  128 one_to_one attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2406  :  128 one_to_one bias_attention node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2407  :  128 one_to_one bias_attention node_embedding 0.01 9 0.3 2
skipped because reset not used
running  2408  :  128 one_to_one bias_attention node_embedding 0.01 9 0.3 3
skipped because reset not used
running  2409  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2410  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2411  :  128 one_to_one bias_attention_reset node_embedding 0.01 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2412  :  128 one_to_one attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.69 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2413  :  128 one_to_one attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  2414  :  128 one_to_one attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  2415  :  128 one_to_one attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.31 GiB already allocated; 381.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2416  :  128 one_to_one attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2417  :  128 one_to_one attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2418  :  128 one_to_one bias_attention agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2419  :  128 one_to_one bias_attention agent_start 0.001 9 0.3 2
skipped because reset not used
running  2420  :  128 one_to_one bias_attention agent_start 0.001 9 0.3 3
skipped because reset not used
running  2421  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2422  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2423  :  128 one_to_one bias_attention_reset agent_start 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2424  :  128 one_to_one attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 725.69 MiB free; 21.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2425  :  128 one_to_one attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  2426  :  128 one_to_one attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  2427  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.28 GiB already allocated; 59.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2428  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 61.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2429  :  128 one_to_one attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 61.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2430  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2431  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.3 2
skipped because reset not used
running  2432  :  128 one_to_one bias_attention last_agent_visited 0.001 9 0.3 3
skipped because reset not used
running  2433  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2434  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2435  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2436  :  128 one_to_one attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.69 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2437  :  128 one_to_one attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  2438  :  128 one_to_one attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  2439  :  128 one_to_one attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.31 GiB already allocated; 381.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2440  :  128 one_to_one attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2441  :  128 one_to_one attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2442  :  128 one_to_one bias_attention node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2443  :  128 one_to_one bias_attention node_embedding 0.001 9 0.3 2
skipped because reset not used
running  2444  :  128 one_to_one bias_attention node_embedding 0.001 9 0.3 3
skipped because reset not used
running  2445  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2446  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2447  :  128 one_to_one bias_attention_reset node_embedding 0.001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2448  :  128 one_to_one attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.69 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2449  :  128 one_to_one attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  2450  :  128 one_to_one attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  2451  :  128 one_to_one attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.31 GiB already allocated; 381.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2452  :  128 one_to_one attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2453  :  128 one_to_one attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2454  :  128 one_to_one bias_attention agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2455  :  128 one_to_one bias_attention agent_start 0.0001 9 0.3 2
skipped because reset not used
running  2456  :  128 one_to_one bias_attention agent_start 0.0001 9 0.3 3
skipped because reset not used
running  2457  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2458  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2459  :  128 one_to_one bias_attention_reset agent_start 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2460  :  128 one_to_one attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 721.69 MiB free; 21.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2461  :  128 one_to_one attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  2462  :  128 one_to_one attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  2463  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 21.28 GiB already allocated; 55.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2464  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2465  :  128 one_to_one attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 20.83 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2466  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2467  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 2
skipped because reset not used
running  2468  :  128 one_to_one bias_attention last_agent_visited 0.0001 9 0.3 3
skipped because reset not used
running  2469  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2470  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2471  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2472  :  128 one_to_one attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 20.69 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2473  :  128 one_to_one attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  2474  :  128 one_to_one attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  2475  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.31 GiB already allocated; 381.69 MiB free; 21.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2476  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 836.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2477  :  128 one_to_one attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 838.00 MiB (GPU 0; 23.70 GiB total capacity; 19.86 GiB already allocated; 383.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2478  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2479  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.3 2
skipped because reset not used
running  2480  :  128 one_to_one bias_attention node_embedding 0.0001 9 0.3 3
skipped because reset not used
running  2481  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2482  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2483  :  128 one_to_one bias_attention_reset node_embedding 0.0001 9 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=9, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.25 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2484  :  128 one_to_one attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.18 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2485  :  128 one_to_one attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  2486  :  128 one_to_one attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  2487  :  128 one_to_one attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2488  :  128 one_to_one attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2489  :  128 one_to_one attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2490  :  128 one_to_one bias_attention agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2491  :  128 one_to_one bias_attention agent_start 0.01 18 0.3 2
skipped because reset not used
running  2492  :  128 one_to_one bias_attention agent_start 0.01 18 0.3 3
skipped because reset not used
running  2493  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2494  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2495  :  128 one_to_one bias_attention_reset agent_start 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2496  :  128 one_to_one attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.22 GiB already allocated; 389.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2497  :  128 one_to_one attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  2498  :  128 one_to_one attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  2499  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 55.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2500  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2501  :  128 one_to_one attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2502  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2503  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.3 2
skipped because reset not used
running  2504  :  128 one_to_one bias_attention last_agent_visited 0.01 18 0.3 3
skipped because reset not used
running  2505  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2506  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2507  :  128 one_to_one bias_attention_reset last_agent_visited 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2508  :  128 one_to_one attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.18 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2509  :  128 one_to_one attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  2510  :  128 one_to_one attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  2511  :  128 one_to_one attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2512  :  128 one_to_one attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2513  :  128 one_to_one attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2514  :  128 one_to_one bias_attention node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2515  :  128 one_to_one bias_attention node_embedding 0.01 18 0.3 2
skipped because reset not used
running  2516  :  128 one_to_one bias_attention node_embedding 0.01 18 0.3 3
skipped because reset not used
running  2517  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2518  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2519  :  128 one_to_one bias_attention_reset node_embedding 0.01 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.01, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2520  :  128 one_to_one attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.18 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2521  :  128 one_to_one attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  2522  :  128 one_to_one attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  2523  :  128 one_to_one attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2524  :  128 one_to_one attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2525  :  128 one_to_one attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2526  :  128 one_to_one bias_attention agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2527  :  128 one_to_one bias_attention agent_start 0.001 18 0.3 2
skipped because reset not used
running  2528  :  128 one_to_one bias_attention agent_start 0.001 18 0.3 3
skipped because reset not used
running  2529  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2530  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2531  :  128 one_to_one bias_attention_reset agent_start 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2532  :  128 one_to_one attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.22 GiB already allocated; 389.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2533  :  128 one_to_one attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  2534  :  128 one_to_one attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  2535  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 55.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2536  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2537  :  128 one_to_one attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2538  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2539  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.3 2
skipped because reset not used
running  2540  :  128 one_to_one bias_attention last_agent_visited 0.001 18 0.3 3
skipped because reset not used
running  2541  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2542  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2543  :  128 one_to_one bias_attention_reset last_agent_visited 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2544  :  128 one_to_one attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.18 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2545  :  128 one_to_one attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  2546  :  128 one_to_one attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  2547  :  128 one_to_one attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2548  :  128 one_to_one attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2549  :  128 one_to_one attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2550  :  128 one_to_one bias_attention node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2551  :  128 one_to_one bias_attention node_embedding 0.001 18 0.3 2
skipped because reset not used
running  2552  :  128 one_to_one bias_attention node_embedding 0.001 18 0.3 3
skipped because reset not used
running  2553  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2554  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2555  :  128 one_to_one bias_attention_reset node_embedding 0.001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2556  :  128 one_to_one attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.18 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2557  :  128 one_to_one attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  2558  :  128 one_to_one attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  2559  :  128 one_to_one attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2560  :  128 one_to_one attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2561  :  128 one_to_one attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2562  :  128 one_to_one bias_attention agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2563  :  128 one_to_one bias_attention agent_start 0.0001 18 0.3 2
skipped because reset not used
running  2564  :  128 one_to_one bias_attention agent_start 0.0001 18 0.3 3
skipped because reset not used
running  2565  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2566  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2567  :  128 one_to_one bias_attention_reset agent_start 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.agent_start: 'agent_start'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2568  :  128 one_to_one attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.22 GiB already allocated; 389.69 MiB free; 21.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2569  :  128 one_to_one attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  2570  :  128 one_to_one attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  2571  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 332.00 MiB (GPU 0; 23.70 GiB total capacity; 21.03 GiB already allocated; 55.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2572  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2573  :  128 one_to_one attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.23 GiB already allocated; 57.69 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2574  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2575  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 2
skipped because reset not used
running  2576  :  128 one_to_one bias_attention last_agent_visited 0.0001 18 0.3 3
skipped because reset not used
running  2577  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2578  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2579  :  128 one_to_one bias_attention_reset last_agent_visited 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2580  :  128 one_to_one attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention: 'attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 21.18 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2581  :  128 one_to_one attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  2582  :  128 one_to_one attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  2583  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 654.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 107.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2584  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2585  :  128 one_to_one attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.attention_reset: 'attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 944.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 109.69 MiB free; 21.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2586  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention: 'bias_attention'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2587  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.3 2
skipped because reset not used
running  2588  :  128 one_to_one bias_attention node_embedding 0.0001 18 0.3 3
skipped because reset not used
running  2589  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 1
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=1, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2590  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 2
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=2, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
running  2591  :  128 one_to_one bias_attention_reset node_embedding 0.0001 18 0.3 3
Namespace(init_strat=<InitStrategy.one_to_one: 'one_to_one'>, transition_strat=<TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>, readout_strat=<ReadOutStrategy.node_embedding: 'node_embedding'>, device=0, seed=0, hidden_units=128, num_agents=169343, num_steps=18, self_loops=True, epochs=2000, reset_neighbourhood_size=3, training_dropout_rate=0.3, reduce_function='log', use_time=True, lr=0.0001, weight_decay=0.01, leakyRELU_neg_slope=0.01, leakyRELU_edge_neg_slope=0.2, use_mlp_input=True, post_ln=False, activation_function='leaky_relu', global_agent_node_update=False, global_agent_agent_update=False, sparse_conv=False, mlp_width_mult=2, attn_width_mult=2, num_workers=0, visited_decay=0.9, test_argmax=False, num_pos_attention_heads=1, warmup=5, gumbel_temp=0.66666667, gumbel_min_temp=0.66666667, gumbel_warmup=-1, gumbel_decay_epochs=100, min_lr_mult=1e-07, job_id=111, dataset='ogbn-arxiv')
got err CUDA out of memory. Tried to allocate 106.83 GiB (GPU 0; 23.70 GiB total capacity; 890.26 MiB already allocated; 20.97 GiB free; 940.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
max result test acc:  [[[[[[[6.90253115e-310 6.90253115e-310 4.66101932e-310]]

     [[4.66101932e-310 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]




  [[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]




  [[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]




  [[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]]





 [[[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]




  [[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]




  [[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]




  [[[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]



   [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]


    [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]

     [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]]]]] 0
--------------------
[32, 64, 128] [0.0, 0.3] [3, 6, 9, 18] [0.01, 0.001, 0.0001] [<ReadOutStrategy.agent_start: 'agent_start'>, <ReadOutStrategy.last_agent_visited: 'last_agent_visited'>, <ReadOutStrategy.node_embedding: 'node_embedding'>] [<TransitionStrategy.attention: 'attention'>, <TransitionStrategy.attention_reset: 'attention_reset'>, <TransitionStrategy.bias_attention: 'bias_attention'>, <TransitionStrategy.bias_attention_reset: 'bias_attention_reset'>] [<InitStrategy.one_to_one: 'one_to_one'>]
--------------------
[[[[[[[[ 6.90253115e-310  6.90253115e-310  4.66101932e-310]]

      [[ 4.66101932e-310  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]]





  [[[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]]]






 [[[[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]]





  [[[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]]]






 [[[[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]]




   [[[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]]



    [[[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]


     [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]

      [[ 0.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]




   [[[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]




   [[[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]]





  [[[[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]




   [[[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]




   [[[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]




   [[[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]



    [[[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]


     [[[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]

      [[-1.00000000e+000 -1.00000000e+000 -1.00000000e+000]]]]]]]]
finished at: Mon Jan 2 17:06:41 CET 2023
